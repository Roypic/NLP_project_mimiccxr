{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e28832b-7f45-4c19-bd70-d90338900774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " NER model loadedï¼šen_core_web_sm\n",
      "\n",
      "===== showcaseï¼ˆtop 5ï¼‰===== \n",
      "\n",
      "Sentence 1:\n",
      "Text: The lungs are otherwise clear with no evidence of focal opacities concerning for infectious process.\n",
      "spaCy NER: []\n",
      "--------------------------------------------------------------------------------\n",
      "Sentence 2:\n",
      "Text: Right PICC line again extends to the cavoatrial junction.\n",
      "spaCy NER: []\n",
      "--------------------------------------------------------------------------------\n",
      "Sentence 3:\n",
      "Text: Streaky opacity at the left lung base thought likely atelectatic in etiology.\n",
      "spaCy NER: [('Streaky', 0, 7, 'PERSON')]\n",
      "--------------------------------------------------------------------------------\n",
      "Sentence 4:\n",
      "Text: No effusion or pneumothorax.\n",
      "spaCy NER: []\n",
      "--------------------------------------------------------------------------------\n",
      "Sentence 5:\n",
      "Text: There is mild obscuration of the right cardiac border, however, no definite densities are appreciated on the lateral view.\n",
      "spaCy NER: []\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      " inference results saved toï¼šspacy_standard_predictions.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import spacy\n",
    "\n",
    "\n",
    "# python -m spacy download en_core_web_sm\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "print(\" NER model loadedï¼šen_core_web_sm\")\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# 2. read json\n",
    "# --------------------------\n",
    "json_path = \"./sentence_chunk_combined_Annotated.json\"\n",
    "\n",
    "with open(json_path, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "annotations = data[\"annotations\"]  # list of [text, ann]\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# 3. NER\n",
    "# --------------------------\n",
    "def run_spacy_inference():\n",
    "    results = []\n",
    "\n",
    "    for item in annotations:\n",
    "        text = item[0]\n",
    "\n",
    "        doc = nlp(text)\n",
    "        ents = [(ent.text, ent.start_char, ent.end_char, ent.label_) for ent in doc.ents]\n",
    "\n",
    "        results.append({\n",
    "            \"text\": text,\n",
    "            \"spacy_entities\": ents\n",
    "        })\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# 4. excute and print\n",
    "# --------------------------\n",
    "output = run_spacy_inference()\n",
    "\n",
    "print(\"\\n===== showcaseï¼ˆtop 5ï¼‰===== \\n\")\n",
    "\n",
    "for i, item in enumerate(output[:5]):\n",
    "    print(f\"Sentence {i+1}:\")\n",
    "    print(\"Text:\", item[\"text\"])\n",
    "    print(\"spaCy NER:\", item[\"spacy_entities\"])\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "\n",
    "import json\n",
    "with open(\"spacy_standard_predictions.json\", \"w\") as f:\n",
    "    json.dump(output, f, indent=2)\n",
    "\n",
    "print(\"\\n inference results saved toï¼šspacy_standard_predictions.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0396e4b0-5cb7-4b93-bea8-c2955d6fc5ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total samples: 100\n",
      "training set 70 ï¼Œevaluation set 30\n",
      "load entity labels: {'DESCRIPTION', 'FINDING', 'OTHER', 'ANATOMY', 'LOCATION'}\n",
      "saved to: train.spacy\n",
      "saved to: eval.spacy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/homefs/hl24f166/.conda/envs/dl_pytorch/lib/python3.9/site-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Compared to the prior radiograph, the right IJ lin...\" with entities \"[[74, 81, 'DESCRIPTION'], [69, 73, 'DESCRIPTION'],...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/storage/homefs/hl24f166/.conda/envs/dl_pytorch/lib/python3.9/site-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Given the acute fluctuations, findings favor mild ...\" with entities \"[[30, 38, 'FINDING'], [45, 49, 'DESCRIPTION'], [63...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/storage/homefs/hl24f166/.conda/envs/dl_pytorch/lib/python3.9/site-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Repeat after treatment to document resolution.\" with entities \"[[34, 45, 'DESCRIPTION'], [0, 6, 'OTHER'], [7, 12,...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/storage/homefs/hl24f166/.conda/envs/dl_pytorch/lib/python3.9/site-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"In comparison with the study of ___, there again a...\" with entities \"[[0, 2, 'OTHER'], [37, 42, 'OTHER'], [43, 48, 'OTH...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/storage/homefs/hl24f166/.conda/envs/dl_pytorch/lib/python3.9/site-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"The cardiac silhouette is mildly enlarged with lef...\" with entities \"[[4, 22, 'ANATOMY'], [47, 51, 'LOCATION'], [0, 3, ...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/storage/homefs/hl24f166/.conda/envs/dl_pytorch/lib/python3.9/site-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"2. Enlarged pulmonary arteries are suspicious for ...\" with entities \"[[35, 45, 'DESCRIPTION'], [50, 72, 'FINDING'], [31...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/storage/homefs/hl24f166/.conda/envs/dl_pytorch/lib/python3.9/site-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"The apperance of the distal left clavicle is stabl...\" with entities \"[[45, 51, 'DESCRIPTION'], [0, 3, 'OTHER'], [14, 16...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/storage/homefs/hl24f166/.conda/envs/dl_pytorch/lib/python3.9/site-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"No free air below the right hemidiaphragm.\" with entities \"[[12, 17, 'OTHER'], [22, 27, 'LOCATION'], [28, 41,...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/storage/homefs/hl24f166/.conda/envs/dl_pytorch/lib/python3.9/site-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"The heart size is normal.\" with entities \"[[4, 9, 'ANATOMY'], [18, 24, 'DESCRIPTION'], [0, 4...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/storage/homefs/hl24f166/.conda/envs/dl_pytorch/lib/python3.9/site-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Left hilar prominence may be a normal findings in ...\" with entities \"[[26, 28, 'DESCRIPTION'], [29, 30, 'DESCRIPTION'],...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20  Losses={'ner': 570.0405077276259}\n",
      "Epoch 2/20  Losses={'ner': 435.57427351048045}\n",
      "Epoch 3/20  Losses={'ner': 355.1196583452005}\n",
      "Epoch 4/20  Losses={'ner': 291.6339738306977}\n",
      "Epoch 5/20  Losses={'ner': 238.81171503647158}\n",
      "Epoch 6/20  Losses={'ner': 193.93953842442215}\n",
      "Epoch 7/20  Losses={'ner': 157.47429326391105}\n",
      "Epoch 8/20  Losses={'ner': 122.23638750723488}\n",
      "Epoch 9/20  Losses={'ner': 105.82948345383785}\n",
      "Epoch 10/20  Losses={'ner': 109.20330382802553}\n",
      "Epoch 11/20  Losses={'ner': 88.21925424025275}\n",
      "Epoch 12/20  Losses={'ner': 96.38008879421672}\n",
      "Epoch 13/20  Losses={'ner': 85.92706416810691}\n",
      "Epoch 14/20  Losses={'ner': 68.22198183644316}\n",
      "Epoch 15/20  Losses={'ner': 70.72751916004425}\n",
      "Epoch 16/20  Losses={'ner': 56.650038067260866}\n",
      "Epoch 17/20  Losses={'ner': 63.05612962746928}\n",
      "Epoch 18/20  Losses={'ner': 49.03098449966949}\n",
      "Epoch 19/20  Losses={'ner': 54.0543704653914}\n",
      "Epoch 20/20  Losses={'ner': 36.38526339944105}\n",
      "\n",
      " saved to ner_model/\n",
      "\n",
      "===== Evaluation Results =====\n",
      "DESCRIPTION   P=0.3125  R=0.3448  F1=0.3279\n",
      "FINDING       P=0.3500  R=0.3415  F1=0.3457\n",
      "OTHER         P=0.2388  R=0.2222  F1=0.2302\n",
      "ANATOMY       P=0.1875  R=0.1429  F1=0.1622\n",
      "LOCATION      P=0.1538  R=0.1333  F1=0.1429\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "from spacy.training.example import Example\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "\n",
    "# --------------------------\n",
    "# 1. load json\n",
    "# --------------------------\n",
    "json_path = \"./sentence_chunk_combined_Annotated.json\"\n",
    "\n",
    "with open(json_path, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "annotations = data[\"annotations\"]  # list of [text, {\"entities\":[...] }]\n",
    "print(\"total samples:\", len(annotations))\n",
    "\n",
    "# --------------------------\n",
    "# 2. dataset splitï¼š70% training / 30% evaluation\n",
    "# --------------------------\n",
    "train_data, eval_data = train_test_split(\n",
    "    annotations, test_size=0.30, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"training set {len(train_data)} ï¼Œevaluation set {len(eval_data)}\")\n",
    "\n",
    "# --------------------------\n",
    "# 3. create spaCy NER label system\n",
    "# --------------------------\n",
    "nlp = spacy.blank(\"en\")   # null model\n",
    "ner = nlp.add_pipe(\"ner\")\n",
    "\n",
    "# æ”¶é›†æ‰€æœ‰æ ‡ç­¾\n",
    "all_labels = set()\n",
    "for text, ann in annotations:\n",
    "    for start, end, label in ann[\"entities\"]:\n",
    "        all_labels.add(label)\n",
    "\n",
    "for label in all_labels:\n",
    "    ner.add_label(label)\n",
    "\n",
    "print(\"load entity labels:\", all_labels)\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# 4. convert data to spaCy DocBin\n",
    "# --------------------------\n",
    "def convert_to_spacy(data, output_path):\n",
    "    db = DocBin()\n",
    "    for text, ann in data:\n",
    "        doc = nlp.make_doc(text)\n",
    "        ents = []\n",
    "        for start, end, label in ann[\"entities\"]:\n",
    "            span = doc.char_span(start, end, label=label)\n",
    "            if span:\n",
    "                ents.append(span)\n",
    "        doc.ents = ents\n",
    "        db.add(doc)\n",
    "    db.to_disk(output_path)\n",
    "    print(\"saved to:\", output_path)\n",
    "\n",
    "convert_to_spacy(train_data, \"train.spacy\")\n",
    "convert_to_spacy(eval_data, \"eval.spacy\")\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# 5. Train Ner model\n",
    "# --------------------------\n",
    "optimizer = nlp.initialize()\n",
    "\n",
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    random.shuffle(train_data)\n",
    "\n",
    "    losses = {}\n",
    "    for text, ann in train_data:\n",
    "        doc = nlp.make_doc(text)\n",
    "        example = Example.from_dict(doc, {\"entities\": ann[\"entities\"]})\n",
    "        nlp.update([example], sgd=optimizer, losses=losses)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}  Losses={losses}\")\n",
    "\n",
    "nlp.to_disk(\"ner_model\")\n",
    "print(\"\\n saved to ner_model/\")\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# 6. evaluation\n",
    "# --------------------------\n",
    "from collections import Counter\n",
    "\n",
    "def evaluate(nlp, eval_data):\n",
    "    tp = Counter()\n",
    "    fp = Counter()\n",
    "    fn = Counter()\n",
    "\n",
    "    for text, ann in eval_data:\n",
    "        doc = nlp(text)\n",
    "\n",
    "        gold_ents = {(start, end, label) for start, end, label in ann[\"entities\"]}\n",
    "        pred_ents = {(ent.start_char, ent.end_char, ent.label_) for ent in doc.ents}\n",
    "\n",
    "        for ent in pred_ents:\n",
    "            if ent in gold_ents:\n",
    "                tp[ent[2]] += 1\n",
    "            else:\n",
    "                fp[ent[2]] += 1\n",
    "\n",
    "        for ent in gold_ents:\n",
    "            if ent not in pred_ents:\n",
    "                fn[ent[2]] += 1\n",
    "\n",
    "    print(\"\\n===== Evaluation Results =====\")\n",
    "    for label in all_labels:\n",
    "        p = tp[label] / (tp[label] + fp[label] + 1e-8)\n",
    "        r = tp[label] / (tp[label] + fn[label] + 1e-8)\n",
    "        f1 = 2 * p * r / (p + r + 1e-8)\n",
    "        print(f\"{label:12s}  P={p:.4f}  R={r:.4f}  F1={f1:.4f}\")\n",
    "\n",
    "\n",
    "evaluate(nlp, eval_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05920b25-9322-4778-a227-89920bc3ae4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loadedï¼šner_model/\n",
      "Evaluation sample number: 30\n",
      "\n",
      "========================================\n",
      "TEXT:\n",
      "In comparison with the study of ___, there again are low lung volumes that accentuate the prominence of the transverse diameter of the heart.\n",
      "----------------------------------------\n",
      "GOLD:\n",
      "  In [OTHER] (pos 0-2)\n",
      "  there [OTHER] (pos 37-42)\n",
      "  again [OTHER] (pos 43-48)\n",
      "  lung [ANATOMY] (pos 57-61)\n",
      "  heart [ANATOMY] (pos 135-140)\n",
      "  comparison [OTHER] (pos 3-13)\n",
      "  with [OTHER] (pos 14-18)\n",
      "  the [OTHER] (pos 19-22)\n",
      "  accentuate [DESCRIPTION] (pos 75-85)\n",
      "  prominence [DESCRIPTION] (pos 90-100)\n",
      "  low  [FINDING] (pos 53-57)\n",
      "  volumes [FINDING] (pos 62-69)\n",
      "  study [OTHER] (pos 23-28)\n",
      "   of [OTHER] (pos 28-31)\n",
      "  ___ [OTHER] (pos 32-35)\n",
      "  are [OTHER] (pos 49-52)\n",
      "  that [OTHER] (pos 70-74)\n",
      "  the [OTHER] (pos 86-89)\n",
      "  of [OTHER] (pos 101-103)\n",
      "  of [OTHER] (pos 128-130)\n",
      "  the [OTHER] (pos 131-134)\n",
      "  the  [OTHER] (pos 104-108)\n",
      "  transverse diameter [LOCATION] (pos 108-127)\n",
      "\n",
      "PRED:\n",
      "  In comparison [OTHER] (pos 0-13)\n",
      "  with [OTHER] (pos 14-18)\n",
      "  the [OTHER] (pos 19-22)\n",
      "  study [OTHER] (pos 23-28)\n",
      "  ___ [OTHER] (pos 32-35)\n",
      "  there [OTHER] (pos 37-42)\n",
      "  again [OTHER] (pos 43-48)\n",
      "  are [OTHER] (pos 49-52)\n",
      "  low [DESCRIPTION] (pos 53-56)\n",
      "  lung [ANATOMY] (pos 57-61)\n",
      "  volumes [FINDING] (pos 62-69)\n",
      "  that [OTHER] (pos 70-74)\n",
      "  accentuate [DESCRIPTION] (pos 75-85)\n",
      "  the [OTHER] (pos 86-89)\n",
      "  prominence [DESCRIPTION] (pos 90-100)\n",
      "  of [OTHER] (pos 101-103)\n",
      "  the [OTHER] (pos 104-107)\n",
      "  transverse diameter [LOCATION] (pos 108-127)\n",
      "  of [OTHER] (pos 128-130)\n",
      "  the [OTHER] (pos 131-134)\n",
      "  heart [ANATOMY] (pos 135-140)\n",
      "\n",
      "MATCH SUMMARY:\n",
      "ðŸŸ© TP = 18 ðŸŸ¦ FP = 3 ðŸŸ¥ FN = 5\n",
      "\u001b[32mTP: with [OTHER]\u001b[0m\n",
      "\u001b[32mTP: the [OTHER]\u001b[0m\n",
      "\u001b[32mTP: study [OTHER]\u001b[0m\n",
      "\u001b[32mTP: ___ [OTHER]\u001b[0m\n",
      "\u001b[32mTP: there [OTHER]\u001b[0m\n",
      "\u001b[32mTP: again [OTHER]\u001b[0m\n",
      "\u001b[32mTP: are [OTHER]\u001b[0m\n",
      "\u001b[32mTP: lung [ANATOMY]\u001b[0m\n",
      "\u001b[32mTP: volumes [FINDING]\u001b[0m\n",
      "\u001b[32mTP: that [OTHER]\u001b[0m\n",
      "\u001b[32mTP: accentuate [DESCRIPTION]\u001b[0m\n",
      "\u001b[32mTP: the [OTHER]\u001b[0m\n",
      "\u001b[32mTP: prominence [DESCRIPTION]\u001b[0m\n",
      "\u001b[32mTP: of [OTHER]\u001b[0m\n",
      "\u001b[32mTP: transverse diameter [LOCATION]\u001b[0m\n",
      "\u001b[32mTP: of [OTHER]\u001b[0m\n",
      "\u001b[32mTP: the [OTHER]\u001b[0m\n",
      "\u001b[32mTP: heart [ANATOMY]\u001b[0m\n",
      "\u001b[34mFP: In comparison [OTHER]\u001b[0m\n",
      "\u001b[34mFP: low [DESCRIPTION]\u001b[0m\n",
      "\u001b[34mFP: the [OTHER]\u001b[0m\n",
      "\u001b[31mFN: In [OTHER]\u001b[0m\n",
      "\u001b[31mFN: comparison [OTHER]\u001b[0m\n",
      "\u001b[31mFN:  of [OTHER]\u001b[0m\n",
      "\u001b[31mFN: low  [FINDING]\u001b[0m\n",
      "\u001b[31mFN: the  [OTHER]\u001b[0m\n",
      "\n",
      "========================================\n",
      "TEXT:\n",
      "In comparison to ___ chest radiograph, the heart size remains normal an the lungs are clear.\n",
      "----------------------------------------\n",
      "GOLD:\n",
      "  In comparison to [DESCRIPTION] (pos 0-16)\n",
      "  ___ [OTHER] (pos 17-20)\n",
      "  chest [ANATOMY] (pos 21-26)\n",
      "  radiograph, the [OTHER] (pos 27-42)\n",
      "  heart [ANATOMY] (pos 43-48)\n",
      "  size remains normal [DESCRIPTION] (pos 49-68)\n",
      "  an the [OTHER] (pos 69-75)\n",
      "  lungs [ANATOMY] (pos 76-81)\n",
      "  are clear [FINDING] (pos 82-91)\n",
      "\n",
      "PRED:\n",
      "  In comparison to [DESCRIPTION] (pos 0-16)\n",
      "  ___ [OTHER] (pos 17-20)\n",
      "  chest [ANATOMY] (pos 21-26)\n",
      "  radiograph, the [OTHER] (pos 27-42)\n",
      "  heart [ANATOMY] (pos 43-48)\n",
      "  size remains normal [DESCRIPTION] (pos 49-68)\n",
      "  an the [OTHER] (pos 69-75)\n",
      "  lungs [ANATOMY] (pos 76-81)\n",
      "  are clear [FINDING] (pos 82-91)\n",
      "\n",
      "MATCH SUMMARY:\n",
      "ðŸŸ© TP = 9 ðŸŸ¦ FP = 0 ðŸŸ¥ FN = 0\n",
      "\u001b[32mTP: In comparison to [DESCRIPTION]\u001b[0m\n",
      "\u001b[32mTP: ___ [OTHER]\u001b[0m\n",
      "\u001b[32mTP: chest [ANATOMY]\u001b[0m\n",
      "\u001b[32mTP: radiograph, the [OTHER]\u001b[0m\n",
      "\u001b[32mTP: heart [ANATOMY]\u001b[0m\n",
      "\u001b[32mTP: size remains normal [DESCRIPTION]\u001b[0m\n",
      "\u001b[32mTP: an the [OTHER]\u001b[0m\n",
      "\u001b[32mTP: lungs [ANATOMY]\u001b[0m\n",
      "\u001b[32mTP: are clear [FINDING]\u001b[0m\n",
      "\n",
      "========================================\n",
      "TEXT:\n",
      "The cardiac, mediastinal and hilar contours are normal.\n",
      "----------------------------------------\n",
      "GOLD:\n",
      "  The [OTHER] (pos 0-3)\n",
      "  cardiac, mediastinal and hilar contours [LOCATION] (pos 4-43)\n",
      "  are [OTHER] (pos 44-47)\n",
      "  normal [DESCRIPTION] (pos 48-54)\n",
      "  . [OTHER] (pos 54-55)\n",
      "\n",
      "PRED:\n",
      "  The [OTHER] (pos 0-3)\n",
      "  cardiac, mediastinal and hilar contours [LOCATION] (pos 4-43)\n",
      "  are [OTHER] (pos 44-47)\n",
      "  normal [DESCRIPTION] (pos 48-54)\n",
      "  . [OTHER] (pos 54-55)\n",
      "\n",
      "MATCH SUMMARY:\n",
      "ðŸŸ© TP = 5 ðŸŸ¦ FP = 0 ðŸŸ¥ FN = 0\n",
      "\u001b[32mTP: The [OTHER]\u001b[0m\n",
      "\u001b[32mTP: cardiac, mediastinal and hilar contours [LOCATION]\u001b[0m\n",
      "\u001b[32mTP: are [OTHER]\u001b[0m\n",
      "\u001b[32mTP: normal [DESCRIPTION]\u001b[0m\n",
      "\u001b[32mTP: . [OTHER]\u001b[0m\n",
      "\n",
      "========================================\n",
      "TEXT:\n",
      "No focal consolidation, pleural effusion or pneumothorax is identified.\n",
      "----------------------------------------\n",
      "GOLD:\n",
      "  No [OTHER] (pos 0-2)\n",
      "  focal consolidation [FINDING] (pos 3-22)\n",
      "  , [OTHER] (pos 22-23)\n",
      "  pleural effusion [FINDING] (pos 24-40)\n",
      "  or [OTHER] (pos 41-43)\n",
      "  pneumothorax [FINDING] (pos 44-56)\n",
      "  is identified. [OTHER] (pos 57-71)\n",
      "\n",
      "PRED:\n",
      "  No [OTHER] (pos 0-2)\n",
      "  focal consolidation [FINDING] (pos 3-22)\n",
      "  , [OTHER] (pos 22-23)\n",
      "  pleural effusion [FINDING] (pos 24-40)\n",
      "  or [OTHER] (pos 41-43)\n",
      "  pneumothorax [FINDING] (pos 44-56)\n",
      "  is identified. [OTHER] (pos 57-71)\n",
      "\n",
      "MATCH SUMMARY:\n",
      "ðŸŸ© TP = 7 ðŸŸ¦ FP = 0 ðŸŸ¥ FN = 0\n",
      "\u001b[32mTP: No [OTHER]\u001b[0m\n",
      "\u001b[32mTP: focal consolidation [FINDING]\u001b[0m\n",
      "\u001b[32mTP: , [OTHER]\u001b[0m\n",
      "\u001b[32mTP: pleural effusion [FINDING]\u001b[0m\n",
      "\u001b[32mTP: or [OTHER]\u001b[0m\n",
      "\u001b[32mTP: pneumothorax [FINDING]\u001b[0m\n",
      "\u001b[32mTP: is identified. [OTHER]\u001b[0m\n",
      "\n",
      "========================================\n",
      "TEXT:\n",
      "No acute cardiopulmonary process.\n",
      "----------------------------------------\n",
      "GOLD:\n",
      "  No [OTHER] (pos 0-2)\n",
      "  acute [DESCRIPTION] (pos 3-8)\n",
      "  cardiopulmonary process [FINDING] (pos 9-32)\n",
      "  . [OTHER] (pos 32-33)\n",
      "\n",
      "PRED:\n",
      "  No [DESCRIPTION] (pos 0-2)\n",
      "  acute [DESCRIPTION] (pos 3-8)\n",
      "  cardiopulmonary [FINDING] (pos 9-24)\n",
      "  process [FINDING] (pos 25-32)\n",
      "\n",
      "MATCH SUMMARY:\n",
      "ðŸŸ© TP = 1 ðŸŸ¦ FP = 3 ðŸŸ¥ FN = 3\n",
      "\u001b[32mTP: acute [DESCRIPTION]\u001b[0m\n",
      "\u001b[34mFP: No [DESCRIPTION]\u001b[0m\n",
      "\u001b[34mFP: cardiopulmonary [FINDING]\u001b[0m\n",
      "\u001b[34mFP: process [FINDING]\u001b[0m\n",
      "\u001b[31mFN: No [OTHER]\u001b[0m\n",
      "\u001b[31mFN: cardiopulmonary process [FINDING]\u001b[0m\n",
      "\u001b[31mFN: . [OTHER]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import spacy\n",
    "from pathlib import Path\n",
    "from termcolor import colored\n",
    "\n",
    "# --------------------------\n",
    "# 1. load model\n",
    "# --------------------------\n",
    "nlp = spacy.load(\"ner_model\")\n",
    "print(\"model loadedï¼šner_model/\")\n",
    "\n",
    "# --------------------------\n",
    "# 2. read in\n",
    "# --------------------------\n",
    "json_path = \"./sentence_chunk_combined_Annotated.json\"\n",
    "with open(json_path, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "annotations = data[\"annotations\"]\n",
    "\n",
    "# 70/30 \n",
    "random.seed(42)\n",
    "random.shuffle(annotations)\n",
    "split = int(len(annotations) * 0.7)\n",
    "eval_data = annotations[split:]\n",
    "\n",
    "print(\"Evaluation sample number:\", len(eval_data))\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# 3.tool box\n",
    "# --------------------------\n",
    "def format_span(text, start, end, label, color):\n",
    "    return (\n",
    "        text[:start]\n",
    "        + colored(text[start:end], color)\n",
    "        + text[end:]\n",
    "        + f\" <{label}>\"\n",
    "    )\n",
    "\n",
    "def showcase_one(text, gold_ents, pred_ents):\n",
    "    print(\"\\n========================================\")\n",
    "    print(\"TEXT:\")\n",
    "    print(text)\n",
    "    print(\"----------------------------------------\")\n",
    "\n",
    "    print(\"GOLD:\")\n",
    "    for (s, e, label) in gold_ents:\n",
    "        print(f\"  {text[s:e]} [{label}] (pos {s}-{e})\")\n",
    "\n",
    "    print(\"\\nPRED:\")\n",
    "    for (s, e, label) in pred_ents:\n",
    "        print(f\"  {text[s:e]} [{label}] (pos {s}-{e})\")\n",
    "\n",
    "    # TP, FP, FN\n",
    "    gold_set = set(gold_ents)\n",
    "    pred_set = set(pred_ents)\n",
    "\n",
    "    tp = gold_set & pred_set\n",
    "    fp = pred_set - gold_set\n",
    "    fn = gold_set - pred_set\n",
    "\n",
    "    print(\"\\nMATCH SUMMARY:\")\n",
    "    print(\"ðŸŸ© TP =\", len(tp), \"ðŸŸ¦ FP =\", len(fp), \"ðŸŸ¥ FN =\", len(fn))\n",
    "\n",
    "\n",
    "    def highlight_all():\n",
    "        chunks = []\n",
    "        for s, e, label in sorted(tp, key=lambda x: x[0]):\n",
    "            print(colored(f\"TP: {text[s:e]} [{label}]\", \"green\"))\n",
    "\n",
    "        for s, e, label in sorted(fp, key=lambda x: x[0]):\n",
    "            print(colored(f\"FP: {text[s:e]} [{label}]\", \"blue\"))\n",
    "\n",
    "        for s, e, label in sorted(fn, key=lambda x: x[0]):\n",
    "            print(colored(f\"FN: {text[s:e]} [{label}]\", \"red\"))\n",
    "\n",
    "    highlight_all()\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# 4. Showcase \n",
    "# --------------------------\n",
    "N = 5  \n",
    "samples = random.sample(eval_data, N)\n",
    "\n",
    "for text, ann in samples:\n",
    "    gold = [(s, e, label) for s, e, label in ann[\"entities\"]]\n",
    "    doc = nlp(text)\n",
    "    pred = [(ent.start_char, ent.end_char, ent.label_) for ent in doc.ents]\n",
    "\n",
    "    showcase_one(text, gold, pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a689f899-4f9d-4a47-9d75-700d00197737",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dl_pytorch)",
   "language": "python",
   "name": "dl_pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
