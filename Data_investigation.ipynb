{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39b10855-83c3-4a2b-9ad4-be4ecff34bd1",
   "metadata": {},
   "source": [
    "# MIMIC-CXR Dataset Sampling Analysis\n",
    "\n",
    "## Dataset Overview\n",
    "We randomly sample **1,000 reports** from **MIMIC-CXR** \n",
    "\n",
    "## Multi-Label Classification Problem\n",
    "Each sampled report is associated with **multiple disease labels**, forming a classic **multi-label classification** task:\n",
    "\n",
    "- A single report may involve several co-occurring diseases or abnormalities (e.g., pneumonia, cardiomegaly).\n",
    "- Labels can be **positive** (present), **negative** (absent), or **uncertain**.\n",
    "\n",
    "## RadGraph Entity Extraction\n",
    "To enhance structure in the textual data, we use **RadGraph** to extract entities and relationships from reports. We focus on the following **four entity types**:\n",
    "\n",
    "- **Anatomy**: Anatomical structures involved (e.g., lungs, heart, bones).\n",
    "- **Finding — Present**: Clearly observed abnormalities or features (e.g., “pulmonary infiltrate”).\n",
    "- **Finding — Absent**: Explicitly ruled-out abnormalities (e.g., “no evidence of pneumonia”).\n",
    "- **Finding — Uncertain**: Ambiguous or possible observations (e.g., “suspected lung shadow”).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2279001b-f5e4-4a17-8016-b93f5864bcf5",
   "metadata": {},
   "source": [
    "## Data Analysis Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "54d5afdd-a40f-4ece-a078-2c476884e603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default matplotlib style\n",
      "[INFO] Loaded dataset with 1000 samples\n",
      "Starting comprehensive data analysis...\n",
      "\n",
      "============================================================\n",
      "BASIC DATASET STATISTICS\n",
      "============================================================\n",
      "Total samples: 1000\n",
      "Number of classes per sample: 75\n",
      "\n",
      "Class distribution:\n",
      "  - Positive samples (1): 4325 total\n",
      "  - Negative samples (-1): 68040 total\n",
      "  - Uncertain samples (0): 2635 total\n",
      "\n",
      "Top 10 most common positive classes:\n",
      "  - Class 73: 468 samples (46.8%)\n",
      "  - Class 0: 431 samples (43.1%)\n",
      "  - Class 1: 322 samples (32.2%)\n",
      "  - Class 9: 255 samples (25.5%)\n",
      "  - Class 74: 227 samples (22.7%)\n",
      "  - Class 8: 208 samples (20.8%)\n",
      "  - Class 12: 198 samples (19.8%)\n",
      "  - Class 17: 146 samples (14.6%)\n",
      "  - Class 19: 133 samples (13.3%)\n",
      "  - Class 13: 123 samples (12.3%)\n",
      "\n",
      "Dataset Balance Analysis:\n",
      "  - Most common class: 468 samples\n",
      "  - Least common class: 1 samples\n",
      "  - Balance ratio: 468.00:1\n",
      "  [WARN] Dataset is HIGHLY IMBALANCED\n",
      "\n",
      "============================================================\n",
      "TEXT LENGTH ANALYSIS\n",
      "============================================================\n",
      "\n",
      "UMLS Entity Text Length Statistics:\n",
      "  - Count: 1000 samples\n",
      "  - Mean: 56.3 words\n",
      "  - Median: 51.0 words\n",
      "  - Min: 3 words\n",
      "  - Max: 182 words\n",
      "  - Std: 26.0 words\n",
      "  - Short (<50 words): 468 (46.8%)\n",
      "  - Medium (50-200 words): 532 (53.2%)\n",
      "  - Long (≥200 words): 0 (0.0%)\n",
      "\n",
      "RadGraph Text Length Statistics:\n",
      "  - Count: 1000 samples\n",
      "  - Mean: 103.9 words\n",
      "  - Median: 97.0 words\n",
      "  - Min: 29 words\n",
      "  - Max: 321 words\n",
      "  - Std: 38.7 words\n",
      "  - Short (<50 words): 17 (1.7%)\n",
      "  - Medium (50-200 words): 954 (95.4%)\n",
      "  - Long (≥200 words): 29 (2.9%)\n",
      "\n",
      "Caption Text Length Statistics:\n",
      "  - Count: 1000 samples\n",
      "  - Mean: 56.3 words\n",
      "  - Median: 51.0 words\n",
      "  - Min: 3 words\n",
      "  - Max: 182 words\n",
      "  - Std: 26.0 words\n",
      "  - Short (<50 words): 468 (46.8%)\n",
      "  - Medium (50-200 words): 532 (53.2%)\n",
      "  - Long (≥200 words): 0 (0.0%)\n",
      "\n",
      "============================================================\n",
      "VOCABULARY AND STYLE ANALYSIS\n",
      "============================================================\n",
      "Analyzing 100 random samples...\n",
      "\n",
      "Vocabulary Analysis:\n",
      "  - Total unique words: 1036\n",
      "  - Total word tokens: 13563\n",
      "  - Average words per text: 67.8\n",
      "\n",
      "Top 20 most common words:\n",
      "  - 'the': 815 times\n",
      "  - 'is': 499 times\n",
      "  - 'of': 390 times\n",
      "  - 'no': 367 times\n",
      "  - 'and': 348 times\n",
      "  - 'are': 236 times\n",
      "  - 'chest': 226 times\n",
      "  - 'there': 205 times\n",
      "  - 'with': 201 times\n",
      "  - 'pleural': 200 times\n",
      "  - 'in': 168 times\n",
      "  - 'right': 167 times\n",
      "  - 'or': 163 times\n",
      "  - 'effusion': 162 times\n",
      "  - 'pneumothorax': 159 times\n",
      "  - 'normal': 157 times\n",
      "  - 'left': 138 times\n",
      "  - 'pulmonary': 132 times\n",
      "  - 'to': 115 times\n",
      "  - 'acute': 105 times\n",
      "\n",
      "Medical Terminology:\n",
      "  - Total medical terms: 545\n",
      "  - Anatomical terms: 198\n",
      "  - Observation terms: 359\n",
      "\n",
      "Top 10 anatomical terms:\n",
      "  - 'pleural': 99 times\n",
      "  - 'right': 69 times\n",
      "  - 'left': 57 times\n",
      "  - 'pulmonary': 56 times\n",
      "  - 'lung': 45 times\n",
      "  - 'lungs': 38 times\n",
      "  - 'silhouette': 35 times\n",
      "  - 'mediastinal': 32 times\n",
      "  - 'size': 32 times\n",
      "  - 'contours': 30 times\n",
      "\n",
      "Top 10 observation terms:\n",
      "  - 'effusion': 82 times\n",
      "  - 'pneumothorax': 76 times\n",
      "  - 'normal': 62 times\n",
      "  - 'acute': 47 times\n",
      "  - 'clear': 44 times\n",
      "  - 'edema': 37 times\n",
      "  - 'atelectasis': 37 times\n",
      "  - 'consolidation': 35 times\n",
      "  - 'process': 32 times\n",
      "  - 'focal': 31 times\n",
      "\n",
      "Writing Style Analysis:\n",
      "  - Common medical phrases:\n",
      "    - 'lung-related': 190 times\n",
      "    - 'is normal': 44 times\n",
      "    - 'heart size': 36 times\n",
      "    - 'no evidence of': 30 times\n",
      "    - 'is clear': 4 times\n",
      "  - Average sentence length: 9.9 words\n",
      "  - Median sentence length: 7.0 words\n",
      "  - Average technical terms per text: 3.2\n",
      "  - Technical terminology density: 32.5%\n",
      "\n",
      "============================================================\n",
      "ENTITY RELATIONSHIP ANALYSIS\n",
      "============================================================\n",
      "Relationship Analysis:\n",
      "  - Total relationships: 19885\n",
      "  - Unique relationship types: 3\n",
      "\n",
      "Top 10 relationship types:\n",
      "  - 'modify': 11940 times\n",
      "  - 'located_at': 7187 times\n",
      "  - 'suggestive_of': 758 times\n",
      "\n",
      "Top 10 entity relationship pairs:\n",
      "  - ANAT-DP --modify--> ANAT-DP: 5448 times\n",
      "  - OBS-DP --located_at--> ANAT-DP: 5393 times\n",
      "  - OBS-DP --modify--> OBS-DP: 4753 times\n",
      "  - OBS-DA --located_at--> ANAT-DP: 1479 times\n",
      "  - OBS-DA --modify--> OBS-DA: 1165 times\n",
      "  - OBS-DP --suggestive_of--> OBS-U: 512 times\n",
      "  - OBS-U --modify--> OBS-U: 360 times\n",
      "  - OBS-U --located_at--> ANAT-DP: 295 times\n",
      "  - OBS-DP --suggestive_of--> OBS-DP: 177 times\n",
      "  - OBS-DP --modify--> OBS-U: 55 times\n",
      "\n",
      "Anatomical-Observation Patterns:\n",
      "  - ANAT-DP: {'OBS-DP': 5455, 'OBS-DA': 1482, 'OBS-U': 306}\n",
      "\n",
      "============================================================\n",
      "GENERATING VISUALIZATIONS\n",
      "============================================================\n",
      "Saved class distribution plot\n",
      "\n",
      "============================================================\n",
      "TEXT LENGTH ANALYSIS\n",
      "============================================================\n",
      "\n",
      "UMLS Entity Text Length Statistics:\n",
      "  - Count: 1000 samples\n",
      "  - Mean: 56.3 words\n",
      "  - Median: 51.0 words\n",
      "  - Min: 3 words\n",
      "  - Max: 182 words\n",
      "  - Std: 26.0 words\n",
      "  - Short (<50 words): 468 (46.8%)\n",
      "  - Medium (50-200 words): 532 (53.2%)\n",
      "  - Long (≥200 words): 0 (0.0%)\n",
      "\n",
      "RadGraph Text Length Statistics:\n",
      "  - Count: 1000 samples\n",
      "  - Mean: 103.9 words\n",
      "  - Median: 97.0 words\n",
      "  - Min: 29 words\n",
      "  - Max: 321 words\n",
      "  - Std: 38.7 words\n",
      "  - Short (<50 words): 17 (1.7%)\n",
      "  - Medium (50-200 words): 954 (95.4%)\n",
      "  - Long (≥200 words): 29 (2.9%)\n",
      "\n",
      "Caption Text Length Statistics:\n",
      "  - Count: 1000 samples\n",
      "  - Mean: 56.3 words\n",
      "  - Median: 51.0 words\n",
      "  - Min: 3 words\n",
      "  - Max: 182 words\n",
      "  - Std: 26.0 words\n",
      "  - Short (<50 words): 468 (46.8%)\n",
      "  - Medium (50-200 words): 532 (53.2%)\n",
      "  - Long (≥200 words): 0 (0.0%)\n",
      "Saved text length distribution plots\n",
      "\n",
      "============================================================\n",
      "COMPREHENSIVE SUMMARY REPORT\n",
      "============================================================\n",
      "Dataset Overview:\n",
      "  - Total samples: 1000\n",
      "  - Number of classes: 75\n",
      "  - Total positive labels: 4325\n",
      "  - Average positive samples per class: 57.7\n",
      "  - Most common class: 468 samples\n",
      "  - Least common class: 1 samples\n",
      "\n",
      "============================================================\n",
      "TEXT LENGTH ANALYSIS\n",
      "============================================================\n",
      "\n",
      "UMLS Entity Text Length Statistics:\n",
      "  - Count: 1000 samples\n",
      "  - Mean: 56.3 words\n",
      "  - Median: 51.0 words\n",
      "  - Min: 3 words\n",
      "  - Max: 182 words\n",
      "  - Std: 26.0 words\n",
      "  - Short (<50 words): 468 (46.8%)\n",
      "  - Medium (50-200 words): 532 (53.2%)\n",
      "  - Long (≥200 words): 0 (0.0%)\n",
      "\n",
      "RadGraph Text Length Statistics:\n",
      "  - Count: 1000 samples\n",
      "  - Mean: 103.9 words\n",
      "  - Median: 97.0 words\n",
      "  - Min: 29 words\n",
      "  - Max: 321 words\n",
      "  - Std: 38.7 words\n",
      "  - Short (<50 words): 17 (1.7%)\n",
      "  - Medium (50-200 words): 954 (95.4%)\n",
      "  - Long (≥200 words): 29 (2.9%)\n",
      "\n",
      "Caption Text Length Statistics:\n",
      "  - Count: 1000 samples\n",
      "  - Mean: 56.3 words\n",
      "  - Median: 51.0 words\n",
      "  - Min: 3 words\n",
      "  - Max: 182 words\n",
      "  - Std: 26.0 words\n",
      "  - Short (<50 words): 468 (46.8%)\n",
      "  - Medium (50-200 words): 532 (53.2%)\n",
      "  - Long (≥200 words): 0 (0.0%)\n",
      "\n",
      "Text Characteristics:\n",
      "  - UMLS Entity: 1000 samples, avg 56.3 words\n",
      "  - RadGraph: 1000 samples, avg 103.9 words\n",
      "  - Caption: 1000 samples, avg 56.3 words\n",
      "\n",
      "============================================================\n",
      "VOCABULARY AND STYLE ANALYSIS\n",
      "============================================================\n",
      "Analyzing 50 random samples...\n",
      "\n",
      "Vocabulary Analysis:\n",
      "  - Total unique words: 793\n",
      "  - Total word tokens: 7284\n",
      "  - Average words per text: 72.8\n",
      "\n",
      "Top 20 most common words:\n",
      "  - 'the': 374 times\n",
      "  - 'is': 304 times\n",
      "  - 'and': 193 times\n",
      "  - 'no': 191 times\n",
      "  - 'of': 151 times\n",
      "  - 'are': 146 times\n",
      "  - 'chest': 122 times\n",
      "  - 'there': 121 times\n",
      "  - 'right': 116 times\n",
      "  - 'pleural': 100 times\n",
      "  - 'with': 97 times\n",
      "  - 'effusion': 90 times\n",
      "  - 'in': 90 times\n",
      "  - 'left': 80 times\n",
      "  - 'or': 69 times\n",
      "  - 'unchanged': 66 times\n",
      "  - 'pneumothorax': 65 times\n",
      "  - 'to': 65 times\n",
      "  - 'on': 62 times\n",
      "  - 'lung': 61 times\n",
      "\n",
      "Medical Terminology:\n",
      "  - Total medical terms: 400\n",
      "  - Anatomical terms: 149\n",
      "  - Observation terms: 256\n",
      "\n",
      "Top 10 anatomical terms:\n",
      "  - 'right': 48 times\n",
      "  - 'pleural': 47 times\n",
      "  - 'left': 36 times\n",
      "  - 'lung': 27 times\n",
      "  - 'pulmonary': 21 times\n",
      "  - 'volumes': 17 times\n",
      "  - 'size': 16 times\n",
      "  - 'silhouette': 15 times\n",
      "  - 'contours': 14 times\n",
      "  - 'lower': 14 times\n",
      "\n",
      "Top 10 observation terms:\n",
      "  - 'effusion': 43 times\n",
      "  - 'unchanged': 33 times\n",
      "  - 'pneumothorax': 32 times\n",
      "  - 'atelectasis': 28 times\n",
      "  - 'normal': 24 times\n",
      "  - 'consolidation': 23 times\n",
      "  - 'acute': 23 times\n",
      "  - 'focal': 21 times\n",
      "  - 'tube': 18 times\n",
      "  - 'process': 16 times\n",
      "\n",
      "Writing Style Analysis:\n",
      "  - Common medical phrases:\n",
      "    - 'lung-related': 81 times\n",
      "    - 'is normal': 24 times\n",
      "    - 'heart size': 20 times\n",
      "    - 'no evidence of': 10 times\n",
      "    - 'is clear': 2 times\n",
      "  - Average sentence length: 10.1 words\n",
      "  - Median sentence length: 7.0 words\n",
      "  - Average technical terms per text: 3.6\n",
      "  - Technical terminology density: 35.9%\n",
      "\n",
      "Vocabulary Characteristics:\n",
      "  - Unique vocabulary size: 793\n",
      "  - Medical terms: 400\n",
      "  - Anatomical terms: 149\n",
      "  - Observation terms: 256\n",
      "\n",
      "Dataset Quality Assessment:\n",
      "  - Missing UMLS data: 0 samples (0.0%)\n",
      "  - Missing RadGraph data: 0 samples (0.0%)\n",
      "  - Data completeness: 100.0%\n",
      "  Data quality: Excellent\n",
      "\n",
      "Key Findings:\n",
      "  - This is a medical imaging dataset with 1000 chest X-ray reports\n",
      "  - Multi-label classification with 75 classes\n",
      "  - Rich medical terminology and entity relationships\n",
      "  - Suitable for medical NLP and computer vision tasks\n",
      "  - Requires domain expertise for proper interpretation\n",
      "\n",
      "Analysis report saved to: ./data/analysis_report.txt\n",
      "\n",
      "Analysis completed successfully!\n",
      "Check './data/analysis_plots/' for visualization files\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter, defaultdict\n",
    "import re\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better plots\n",
    "try:\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    print(\"Using seaborn-v0_8-whitegrid style\")\n",
    "except:\n",
    "    try:\n",
    "        plt.style.use('seaborn-v0_8')\n",
    "        print(\"Using seaborn-v0_8 style\")\n",
    "    except:\n",
    "        plt.style.use('default')\n",
    "        print(\"Using default matplotlib style\")\n",
    "\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "class MIMICDataAnalyzer:\n",
    "    def __init__(self, csv_path, output_file=None):\n",
    "        \"\"\"Initialize the analyzer with the CSV data\"\"\"\n",
    "        self.csv_path = csv_path\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.class_stats = {}\n",
    "        self.text_stats = {}\n",
    "        self.entity_stats = {}\n",
    "        self.output_file = output_file\n",
    "        self.output_lines = []\n",
    "        \n",
    "        self.log(f\"[INFO] Loaded dataset with {len(self.df)} samples\")\n",
    "    \n",
    "    def log(self, message):\n",
    "        \"\"\"Log message to both console and output file\"\"\"\n",
    "        print(message)\n",
    "        if self.output_file:\n",
    "            self.output_lines.append(message)\n",
    "        \n",
    "    def analyze_basic_statistics(self):\n",
    "        \"\"\"Calculate basic statistics about the dataset\"\"\"\n",
    "        self.log(\"\\n\" + \"=\"*60)\n",
    "        self.log(\"BASIC DATASET STATISTICS\")\n",
    "        self.log(\"=\"*60)\n",
    "        \n",
    "        # Basic counts\n",
    "        total_samples = len(self.df)\n",
    "        self.log(f\"Total samples: {total_samples}\")\n",
    "        \n",
    "        # Parse class lists and analyze\n",
    "        class_lists = []\n",
    "        for idx, row in self.df.iterrows():\n",
    "            try:\n",
    "                class_list = json.loads(row['class_list'])\n",
    "                class_lists.append(class_list)\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        if class_lists:\n",
    "            class_array = np.array(class_lists)\n",
    "            self.log(f\"Number of classes per sample: {class_array.shape[1]}\")\n",
    "            \n",
    "            # Count positive samples per class\n",
    "            positive_counts = np.sum(class_array == 1, axis=0)\n",
    "            negative_counts = np.sum(class_array == -1, axis=0)\n",
    "            uncertain_counts = np.sum(class_array == 0, axis=0)\n",
    "            \n",
    "            self.log(f\"\\nClass distribution:\")\n",
    "            self.log(f\"  - Positive samples (1): {np.sum(positive_counts)} total\")\n",
    "            self.log(f\"  - Negative samples (-1): {np.sum(negative_counts)} total\") \n",
    "            self.log(f\"  - Uncertain samples (0): {np.sum(uncertain_counts)} total\")\n",
    "            \n",
    "            # Find most/least common classes\n",
    "            class_indices = np.argsort(positive_counts)[::-1]\n",
    "            self.log(f\"\\nTop 10 most common positive classes:\")\n",
    "            for i in range(min(10, len(class_indices))):\n",
    "                class_idx = class_indices[i]\n",
    "                count = positive_counts[class_idx]\n",
    "                percentage = (count / total_samples) * 100\n",
    "                self.log(f\"  - Class {class_idx}: {count} samples ({percentage:.1f}%)\")\n",
    "            \n",
    "            # Dataset balance analysis\n",
    "            max_positive = np.max(positive_counts)\n",
    "            min_positive = np.min(positive_counts[positive_counts > 0]) if np.any(positive_counts > 0) else 0\n",
    "            balance_ratio = max_positive / min_positive if min_positive > 0 else float('inf')\n",
    "            \n",
    "            self.log(f\"\\nDataset Balance Analysis:\")\n",
    "            self.log(f\"  - Most common class: {max_positive} samples\")\n",
    "            self.log(f\"  - Least common class: {min_positive} samples\")\n",
    "            self.log(f\"  - Balance ratio: {balance_ratio:.2f}:1\")\n",
    "            \n",
    "            if balance_ratio > 10:\n",
    "                self.log(f\"  [WARN] Dataset is HIGHLY IMBALANCED\")\n",
    "            elif balance_ratio > 5:\n",
    "                self.log(f\"  [WARN] Dataset is moderately imbalanced\")\n",
    "            else:\n",
    "                self.log(f\"  [OK] Dataset is relatively balanced\")\n",
    "        \n",
    "        return class_lists\n",
    "    \n",
    "    def analyze_text_lengths(self):\n",
    "        \"\"\"Analyze text lengths in UMLS and RadGraph data\"\"\"\n",
    "        self.log(\"\\n\" + \"=\"*60)\n",
    "        self.log(\"TEXT LENGTH ANALYSIS\")\n",
    "        self.log(\"=\"*60)\n",
    "        \n",
    "        umls_lengths = []\n",
    "        radgraph_lengths = []\n",
    "        caption_lengths = []\n",
    "        \n",
    "        for idx, row in self.df.iterrows():\n",
    "            try:\n",
    "                # UMLS text analysis\n",
    "                umls_data = json.loads(row['umls_json_info'])\n",
    "                if 'caption' in umls_data:\n",
    "                    captions = umls_data['caption']\n",
    "                    if isinstance(captions, list):\n",
    "                        caption_text = ' '.join(captions)\n",
    "                        caption_lengths.append(len(caption_text.split()))\n",
    "                    else:\n",
    "                        caption_lengths.append(len(captions.split()))\n",
    "                \n",
    "                # RadGraph text analysis\n",
    "                radgraph_data = json.loads(row['radgraph_json_info'])\n",
    "                if 'text' in radgraph_data:\n",
    "                    radgraph_text = radgraph_data['text']\n",
    "                    radgraph_lengths.append(len(radgraph_text.split()))\n",
    "                \n",
    "                # UMLS entity text analysis\n",
    "                if 'entities' in umls_data:\n",
    "                    entity_texts = []\n",
    "                    for entity_group in umls_data['entities']:\n",
    "                        if 'caption' in entity_group:\n",
    "                            entity_texts.append(entity_group['caption'])\n",
    "                    if entity_texts:\n",
    "                        combined_entity_text = ' '.join(entity_texts)\n",
    "                        umls_lengths.append(len(combined_entity_text.split()))\n",
    "                        \n",
    "            except Exception:\n",
    "                continue\n",
    "        \n",
    "        # Calculate statistics\n",
    "        def print_text_stats(name, lengths):\n",
    "            if lengths:\n",
    "                self.log(f\"\\n{name} Text Length Statistics:\")\n",
    "                self.log(f\"  - Count: {len(lengths)} samples\")\n",
    "                self.log(f\"  - Mean: {np.mean(lengths):.1f} words\")\n",
    "                self.log(f\"  - Median: {np.median(lengths):.1f} words\")\n",
    "                self.log(f\"  - Min: {np.min(lengths)} words\")\n",
    "                self.log(f\"  - Max: {np.max(lengths)} words\")\n",
    "                self.log(f\"  - Std: {np.std(lengths):.1f} words\")\n",
    "                \n",
    "                # Length distribution\n",
    "                short = sum(1 for x in lengths if x < 50)\n",
    "                medium = sum(1 for x in lengths if 50 <= x < 200)\n",
    "                long = sum(1 for x in lengths if x >= 200)\n",
    "                \n",
    "                self.log(f\"  - Short (<50 words): {short} ({short/len(lengths)*100:.1f}%)\")\n",
    "                self.log(f\"  - Medium (50-200 words): {medium} ({medium/len(lengths)*100:.1f}%)\")\n",
    "                self.log(f\"  - Long (≥200 words): {long} ({long/len(lengths)*100:.1f}%)\")\n",
    "            else:\n",
    "                self.log(f\"\\n{name}: No data available\")\n",
    "        \n",
    "        print_text_stats(\"UMLS Entity\", umls_lengths)\n",
    "        print_text_stats(\"RadGraph\", radgraph_lengths)\n",
    "        print_text_stats(\"Caption\", caption_lengths)\n",
    "        \n",
    "        return {\n",
    "            'umls_lengths': umls_lengths,\n",
    "            'radgraph_lengths': radgraph_lengths,\n",
    "            'caption_lengths': caption_lengths\n",
    "        }\n",
    "    \n",
    "    def analyze_vocabulary_and_style(self, num_samples=100):\n",
    "        \"\"\"Analyze vocabulary, style, and patterns in the text\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"VOCABULARY AND STYLE ANALYSIS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Sample random texts for analysis\n",
    "        sample_indices = np.random.choice(len(self.df), min(num_samples, len(self.df)), replace=False)\n",
    "        \n",
    "        all_texts = []\n",
    "        medical_terms = []\n",
    "        anatomical_terms = []\n",
    "        observation_terms = []\n",
    "        \n",
    "        print(f\"Analyzing {len(sample_indices)} random samples...\")\n",
    "        \n",
    "        for idx in sample_indices:\n",
    "            row = self.df.iloc[idx]\n",
    "            try:\n",
    "                # Get RadGraph text\n",
    "                radgraph_data = json.loads(row['radgraph_json_info'])\n",
    "                if 'text' in radgraph_data:\n",
    "                    text = radgraph_data['text']\n",
    "                    all_texts.append(text)\n",
    "                    \n",
    "                    # Extract medical terms from RadGraph entities\n",
    "                    if 'entities' in radgraph_data:\n",
    "                        for _, entity_info in radgraph_data['entities'].items():\n",
    "                            if 'tokens' in entity_info:\n",
    "                                medical_terms.append(entity_info['tokens'])\n",
    "                                if entity_info.get('label', '').startswith('ANAT'):\n",
    "                                    anatomical_terms.append(entity_info['tokens'])\n",
    "                                elif entity_info.get('label', '').startswith('OBS'):\n",
    "                                    observation_terms.append(entity_info['tokens'])\n",
    "                \n",
    "                # Get UMLS captions\n",
    "                umls_data = json.loads(row['umls_json_info'])\n",
    "                if 'caption' in umls_data and isinstance(umls_data['caption'], list):\n",
    "                    caption_text = ' '.join(umls_data['caption'])\n",
    "                    all_texts.append(caption_text)\n",
    "                    \n",
    "            except Exception:\n",
    "                continue\n",
    "        \n",
    "        # Vocabulary analysis\n",
    "        all_words = []\n",
    "        for text in all_texts:\n",
    "            words = re.findall(r'\\b[a-zA-Z]+\\b', text.lower())\n",
    "            all_words.extend(words)\n",
    "        \n",
    "        word_freq = Counter(all_words)\n",
    "        vocab_size = len(word_freq)\n",
    "        \n",
    "        print(f\"\\nVocabulary Analysis:\")\n",
    "        print(f\"  - Total unique words: {vocab_size}\")\n",
    "        print(f\"  - Total word tokens: {len(all_words)}\")\n",
    "        print(f\"  - Average words per text: {len(all_words) / len(all_texts):.1f}\")\n",
    "        \n",
    "        # Most common words\n",
    "        print(f\"\\nTop 20 most common words:\")\n",
    "        for word, count in word_freq.most_common(20):\n",
    "            print(f\"  - '{word}': {count} times\")\n",
    "        \n",
    "        # Medical terminology analysis\n",
    "        medical_freq = Counter(medical_terms)\n",
    "        anatomical_freq = Counter(anatomical_terms)\n",
    "        observation_freq = Counter(observation_terms)\n",
    "        \n",
    "        print(f\"\\nMedical Terminology:\")\n",
    "        print(f\"  - Total medical terms: {len(medical_freq)}\")\n",
    "        print(f\"  - Anatomical terms: {len(anatomical_freq)}\")\n",
    "        print(f\"  - Observation terms: {len(observation_freq)}\")\n",
    "        \n",
    "        print(f\"\\nTop 10 anatomical terms:\")\n",
    "        for term, count in anatomical_freq.most_common(10):\n",
    "            print(f\"  - '{term}': {count} times\")\n",
    "        \n",
    "        print(f\"\\nTop 10 observation terms:\")\n",
    "        for term, count in observation_freq.most_common(10):\n",
    "            print(f\"  - '{term}': {count} times\")\n",
    "        \n",
    "        # Style analysis\n",
    "        self._analyze_writing_style(all_texts)\n",
    "        \n",
    "        return {\n",
    "            'vocab_size': vocab_size,\n",
    "            'word_freq': word_freq,\n",
    "            'medical_terms': medical_freq,\n",
    "            'anatomical_terms': anatomical_freq,\n",
    "            'observation_terms': observation_freq\n",
    "        }\n",
    "    \n",
    "    def _analyze_writing_style(self, texts):\n",
    "        \"\"\"Analyze writing style patterns\"\"\"\n",
    "        print(f\"\\nWriting Style Analysis:\")\n",
    "        \n",
    "        # Common phrases and patterns\n",
    "        common_phrases = []\n",
    "        sentence_patterns = []\n",
    "        \n",
    "        for text in texts:\n",
    "            sentences = re.split(r'[.!?]+', text)\n",
    "            for sentence in sentences:\n",
    "                sentence = sentence.strip()\n",
    "                if len(sentence) > 10:  # Filter out very short sentences\n",
    "                    sentence_patterns.append(sentence)\n",
    "                    \n",
    "                    # Look for common medical phrases\n",
    "                    if 'no evidence of' in sentence.lower():\n",
    "                        common_phrases.append('no evidence of')\n",
    "                    if 'is normal' in sentence.lower():\n",
    "                        common_phrases.append('is normal')\n",
    "                    if 'is clear' in sentence.lower():\n",
    "                        common_phrases.append('is clear')\n",
    "                    if 'heart size' in sentence.lower():\n",
    "                        common_phrases.append('heart size')\n",
    "                    if 'lung' in sentence.lower():\n",
    "                        common_phrases.append('lung-related')\n",
    "        \n",
    "        phrase_freq = Counter(common_phrases)\n",
    "        print(f\"  - Common medical phrases:\")\n",
    "        for phrase, count in phrase_freq.most_common(10):\n",
    "            print(f\"    - '{phrase}': {count} times\")\n",
    "        \n",
    "        # Sentence length analysis\n",
    "        sentence_lengths = [len(sentence.split()) for sentence in sentence_patterns]\n",
    "        if sentence_lengths:\n",
    "            print(f\"  - Average sentence length: {np.mean(sentence_lengths):.1f} words\")\n",
    "            print(f\"  - Median sentence length: {np.median(sentence_lengths):.1f} words\")\n",
    "        \n",
    "        # Technical terminology density\n",
    "        technical_words = ['opacity', 'atelectasis', 'effusion', 'pneumothorax', 'cardiomegaly', \n",
    "                          'mediastinal', 'hilar', 'hemidiaphragm', 'consolidation', 'infiltrate']\n",
    "        \n",
    "        tech_word_counts = []\n",
    "        for text in texts:\n",
    "            text_lower = text.lower()\n",
    "            tech_count = sum(1 for word in technical_words if word in text_lower)\n",
    "            tech_word_counts.append(tech_count)\n",
    "        \n",
    "        if tech_word_counts:\n",
    "            avg_tech_density = np.mean(tech_word_counts)\n",
    "            print(f\"  - Average technical terms per text: {avg_tech_density:.1f}\")\n",
    "            print(f\"  - Technical terminology density: {avg_tech_density/len(technical_words)*100:.1f}%\")\n",
    "    \n",
    "    def analyze_entity_relationships(self):\n",
    "        \"\"\"Analyze entity relationships and patterns\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ENTITY RELATIONSHIP ANALYSIS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        relationship_types = Counter()\n",
    "        entity_pairs = []\n",
    "        anatomical_observations = defaultdict(list)\n",
    "        \n",
    "        for idx, row in self.df.iterrows():\n",
    "            try:\n",
    "                radgraph_data = json.loads(row['radgraph_json_info'])\n",
    "                if 'entities' in radgraph_data:\n",
    "                    entities = radgraph_data['entities']\n",
    "                    \n",
    "                    # Analyze relationships\n",
    "                    for entity_id, entity_info in entities.items():\n",
    "                        if 'relations' in entity_info:\n",
    "                            for relation in entity_info['relations']:\n",
    "                                if len(relation) >= 2:\n",
    "                                    rel_type = relation[0]\n",
    "                                    relationship_types[rel_type] += 1\n",
    "                                    \n",
    "                                    # Get entity pairs\n",
    "                                    if len(relation) >= 2:\n",
    "                                        target_id = relation[1]\n",
    "                                        if target_id in entities:\n",
    "                                            source_label = entity_info.get('label', '')\n",
    "                                            target_label = entities[target_id].get('label', '')\n",
    "                                            entity_pairs.append((source_label, target_label, rel_type))\n",
    "                                            \n",
    "                                            # Group anatomical-observation pairs\n",
    "                                            if source_label.startswith('ANAT') and target_label.startswith('OBS'):\n",
    "                                                anatomical_observations[source_label].append(target_label)\n",
    "                                            elif target_label.startswith('ANAT') and source_label.startswith('OBS'):\n",
    "                                                anatomical_observations[target_label].append(source_label)\n",
    "                                                \n",
    "            except Exception:\n",
    "                continue\n",
    "        \n",
    "        print(f\"Relationship Analysis:\")\n",
    "        print(f\"  - Total relationships: {sum(relationship_types.values())}\")\n",
    "        print(f\"  - Unique relationship types: {len(relationship_types)}\")\n",
    "        \n",
    "        print(f\"\\nTop 10 relationship types:\")\n",
    "        for rel_type, count in relationship_types.most_common(10):\n",
    "            print(f\"  - '{rel_type}': {count} times\")\n",
    "        \n",
    "        # Most common entity pairs\n",
    "        pair_freq = Counter(entity_pairs)\n",
    "        print(f\"\\nTop 10 entity relationship pairs:\")\n",
    "        for (source, target, rel), count in pair_freq.most_common(10):\n",
    "            print(f\"  - {source} --{rel}--> {target}: {count} times\")\n",
    "        \n",
    "        # Anatomical-observation patterns\n",
    "        print(f\"\\nAnatomical-Observation Patterns:\")\n",
    "        for anat, observations in list(anatomical_observations.items())[:10]:\n",
    "            obs_freq = Counter(observations)\n",
    "            print(f\"  - {anat}: {dict(obs_freq.most_common(3))}\")\n",
    "    \n",
    "    def generate_visualizations(self, output_dir='./data/analysis_plots'):\n",
    "        \"\"\"Generate visualization plots\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"GENERATING VISUALIZATIONS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        Path(output_dir).mkdir(exist_ok=True)\n",
    "        \n",
    "        # 1. Class distribution heatmap\n",
    "        try:\n",
    "            class_lists = []\n",
    "            for idx, row in self.df.iterrows():\n",
    "                try:\n",
    "                    class_list = json.loads(row['class_list'])\n",
    "                    class_lists.append(class_list)\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            if class_lists:\n",
    "                class_array = np.array(class_lists)\n",
    "                positive_counts = np.sum(class_array == 1, axis=0)\n",
    "                \n",
    "                plt.figure(figsize=(15, 8))\n",
    "                plt.bar(range(len(positive_counts)), positive_counts)\n",
    "                plt.title('Class Distribution - Positive Samples per Class')\n",
    "                plt.xlabel('Class Index')\n",
    "                plt.ylabel('Number of Positive Samples')\n",
    "                plt.xticks(rotation=45)\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(f'{output_dir}/class_distribution.png', dpi=300, bbox_inches='tight')\n",
    "                plt.close()\n",
    "                print(\"Saved class distribution plot\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating class distribution plot: {e}\")\n",
    "        \n",
    "        # 2. Text length distributions\n",
    "        try:\n",
    "            text_stats = self.analyze_text_lengths()\n",
    "            \n",
    "            fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "            \n",
    "            for i, (name, lengths) in enumerate([\n",
    "                ('UMLS Entity', text_stats['umls_lengths']),\n",
    "                ('RadGraph', text_stats['radgraph_lengths']),\n",
    "                ('Caption', text_stats['caption_lengths'])\n",
    "            ]):\n",
    "                if lengths:\n",
    "                    axes[i].hist(lengths, bins=30, alpha=0.7, edgecolor='black')\n",
    "                    axes[i].set_title(f'{name} Text Length Distribution')\n",
    "                    axes[i].set_xlabel('Number of Words')\n",
    "                    axes[i].set_ylabel('Frequency')\n",
    "                    axes[i].axvline(np.mean(lengths), color='red', linestyle='--', \n",
    "                                  label=f'Mean: {np.mean(lengths):.1f}')\n",
    "                    axes[i].legend()\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f'{output_dir}/text_length_distributions.png', dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            print(\"Saved text length distribution plots\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating text length plots: {e}\")\n",
    "    \n",
    "    def generate_summary_report(self):\n",
    "        \"\"\"Generate a comprehensive summary report\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"COMPREHENSIVE SUMMARY REPORT\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Basic statistics\n",
    "        total_samples = len(self.df)\n",
    "        \n",
    "        # Class analysis\n",
    "        class_lists = []\n",
    "        for idx, row in self.df.iterrows():\n",
    "            try:\n",
    "                class_list = json.loads(row['class_list'])\n",
    "                class_lists.append(class_list)\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        if class_lists:\n",
    "            class_array = np.array(class_lists)\n",
    "            positive_counts = np.sum(class_array == 1, axis=0)\n",
    "            num_classes = class_array.shape[1]\n",
    "            total_positive = np.sum(positive_counts)\n",
    "            avg_positive_per_class = np.mean(positive_counts)\n",
    "            \n",
    "            print(f\"Dataset Overview:\")\n",
    "            print(f\"  - Total samples: {total_samples}\")\n",
    "            print(f\"  - Number of classes: {num_classes}\")\n",
    "            print(f\"  - Total positive labels: {total_positive}\")\n",
    "            print(f\"  - Average positive samples per class: {avg_positive_per_class:.1f}\")\n",
    "            print(f\"  - Most common class: {np.max(positive_counts)} samples\")\n",
    "            print(f\"  - Least common class: {np.min(positive_counts[positive_counts > 0])} samples\")\n",
    "        \n",
    "        # Text analysis summary\n",
    "        text_stats = self.analyze_text_lengths()\n",
    "        \n",
    "        print(f\"\\nText Characteristics:\")\n",
    "        for name, lengths in [\n",
    "            ('UMLS Entity', text_stats['umls_lengths']),\n",
    "            ('RadGraph', text_stats['radgraph_lengths']),\n",
    "            ('Caption', text_stats['caption_lengths'])\n",
    "        ]:\n",
    "            if lengths:\n",
    "                print(f\"  - {name}: {len(lengths)} samples, avg {np.mean(lengths):.1f} words\")\n",
    "        \n",
    "        # Vocabulary summary\n",
    "        vocab_analysis = self.analyze_vocabulary_and_style(50)  # Sample 50 for speed\n",
    "        \n",
    "        print(f\"\\nVocabulary Characteristics:\")\n",
    "        print(f\"  - Unique vocabulary size: {vocab_analysis['vocab_size']}\")\n",
    "        print(f\"  - Medical terms: {len(vocab_analysis['medical_terms'])}\")\n",
    "        print(f\"  - Anatomical terms: {len(vocab_analysis['anatomical_terms'])}\")\n",
    "        print(f\"  - Observation terms: {len(vocab_analysis['observation_terms'])}\")\n",
    "        \n",
    "        # Dataset quality assessment\n",
    "        print(f\"\\nDataset Quality Assessment:\")\n",
    "        \n",
    "        # Check for missing data\n",
    "        missing_umls = sum(1 for _, row in self.df.iterrows() \n",
    "                          if pd.isna(row['umls_json_info']) or row['umls_json_info'] == '')\n",
    "        missing_radgraph = sum(1 for _, row in self.df.iterrows() \n",
    "                              if pd.isna(row['radgraph_json_info']) or row['radgraph_json_info'] == '')\n",
    "        \n",
    "        print(f\"  - Missing UMLS data: {missing_umls} samples ({missing_umls/total_samples*100:.1f}%)\")\n",
    "        print(f\"  - Missing RadGraph data: {missing_radgraph} samples ({missing_radgraph/total_samples*100:.1f}%)\")\n",
    "        \n",
    "        # Data completeness\n",
    "        completeness = ((total_samples - missing_umls - missing_radgraph) / total_samples) * 100\n",
    "        print(f\"  - Data completeness: {completeness:.1f}%\")\n",
    "        \n",
    "        if completeness > 95:\n",
    "            print(f\"  Data quality: Excellent\")\n",
    "        elif completeness > 90:\n",
    "            print(f\"  Data quality: Good\")\n",
    "        elif completeness > 80:\n",
    "            print(f\"  Data quality: Moderate\")\n",
    "        else:\n",
    "            print(f\"  Data quality: Poor\")\n",
    "        \n",
    "        print(f\"\\nKey Findings:\")\n",
    "        print(f\"  - This is a medical imaging dataset with {total_samples} chest X-ray reports\")\n",
    "        print(f\"  - Multi-label classification with {num_classes} classes\")\n",
    "        print(f\"  - Rich medical terminology and entity relationships\")\n",
    "        print(f\"  - Suitable for medical NLP and computer vision tasks\")\n",
    "        print(f\"  - Requires domain expertise for proper interpretation\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main analysis function\"\"\"\n",
    "    csv_path = './data/sampled_1000_data.csv'\n",
    "    output_file = './data/analysis_report.txt'\n",
    "    \n",
    "    if not Path(csv_path).exists():\n",
    "        print(f\"Error: CSV file not found: {csv_path}\")\n",
    "        return\n",
    "    \n",
    "    # Initialize analyzer with output file\n",
    "    analyzer = MIMICDataAnalyzer(csv_path, output_file)\n",
    "    \n",
    "    # Run comprehensive analysis\n",
    "    analyzer.log(\"Starting comprehensive data analysis...\")\n",
    "    \n",
    "    # 1. Basic statistics\n",
    "    analyzer.analyze_basic_statistics()\n",
    "    \n",
    "    # 2. Text length analysis\n",
    "    analyzer.analyze_text_lengths()\n",
    "    \n",
    "    # 3. Vocabulary and style analysis\n",
    "    analyzer.analyze_vocabulary_and_style()\n",
    "    \n",
    "    # 4. Entity relationship analysis\n",
    "    analyzer.analyze_entity_relationships()\n",
    "    \n",
    "    # 5. Generate visualizations\n",
    "    analyzer.generate_visualizations()\n",
    "    \n",
    "    # 6. Generate summary report\n",
    "    analyzer.generate_summary_report()\n",
    "    \n",
    "    # Save output to file\n",
    "    if analyzer.output_file:\n",
    "        with open(analyzer.output_file, 'w', encoding='utf-8') as f:\n",
    "            f.write('\\n'.join(analyzer.output_lines))\n",
    "        print(f\"\\nAnalysis report saved to: {output_file}\")\n",
    "    \n",
    "    print(\"\\nAnalysis completed successfully!\")\n",
    "    print(\"Check './data/analysis_plots/' for visualization files\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1205f05-45f9-4a0c-8ced-28435bd085f1",
   "metadata": {},
   "source": [
    "##  Processing Pipeline Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "124065e5-a5a6-4b22-9a58-d6951869fd11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up NLTK data packages...\n",
      "punkt already available\n",
      "stopwords already available\n",
      "Downloading NLTK package: wordnet\n",
      "wordnet downloaded but verification failed\n",
      "punkt_tab downloaded (if available)\n",
      "NLTK data setup completed\n",
      "Starting MIMIC-CXR Caption Processing Pipeline\n",
      "============================================================\n",
      "Loading data from CSV...\n",
      "Loaded 1000 samples\n",
      "Extracting captions from UMLS data...\n",
      "Extracted captions from 1000 samples\n",
      "Processing captions through pipeline...\n",
      "  Processing sample 0/1000\n",
      "  Processing sample 100/1000\n",
      "  Processing sample 200/1000\n",
      "  Processing sample 300/1000\n",
      "  Processing sample 400/1000\n",
      "  Processing sample 500/1000\n",
      "  Processing sample 600/1000\n",
      "  Processing sample 700/1000\n",
      "  Processing sample 800/1000\n",
      "  Processing sample 900/1000\n",
      "Caption processing completed\n",
      "Calculating processing statistics...\n",
      "Statistics calculated\n",
      "\n",
      "============================================================\n",
      "TEXT PROCESSING STATISTICS\n",
      "============================================================\n",
      "Dataset Overview:\n",
      "  - Total samples: 1000\n",
      "  - Non-empty raw captions: 1000\n",
      "  - Non-empty processed captions: 1000\n",
      "  - Processing retention rate: 100.0%\n",
      "\n",
      "Length Statistics:\n",
      "  Raw captions:\n",
      "    - Mean: 56.3 words\n",
      "    - Median: 51.0 words\n",
      "    - Min: 3 words\n",
      "    - Max: 182 words\n",
      "  Processed captions:\n",
      "    - Mean: 35.9 words\n",
      "    - Median: 32.0 words\n",
      "    - Min: 2 words\n",
      "    - Max: 125 words\n",
      "\n",
      "Vocabulary Statistics:\n",
      "  - Unique tokens: 1867\n",
      "  - Total tokens: 35887\n",
      "  - Average tokens per caption: 35.9\n",
      "\n",
      "Top 20 Most Common Tokens:\n",
      "  - 'effusion': 978 times\n",
      "  - 'pleural': 940 times\n",
      "  - 'right': 911 times\n",
      "  - 'lung': 904 times\n",
      "  - 'left': 723 times\n",
      "  - 'pneumothorax': 709 times\n",
      "  - 'normal': 586 times\n",
      "  - 'pulmonary': 571 times\n",
      "  - 'acute': 452 times\n",
      "  - 'chest': 434 times\n",
      "  - 'seen': 416 times\n",
      "  - 'atelectasis': 406 times\n",
      "  - 'consolidation': 399 times\n",
      "  - 'silhouette': 385 times\n",
      "  - 'edema': 360 times\n",
      "  - 'unchanged': 356 times\n",
      "  - 'focal': 349 times\n",
      "  - 'clear': 342 times\n",
      "  - 'opacity': 342 times\n",
      "  - 'mild': 321 times\n",
      "Processed data saved to: data/processed_text/processed_captions.csv\n",
      "Statistics saved to: data/processed_text/processing_statistics.json\n",
      "\n",
      "Text processing pipeline completed successfully!\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Structured and Configurable Text Processing Pipeline for MIMIC-CXR Caption Data\n",
    "Processes captions from UMLS JSON data with configurable preprocessing steps\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional, Callable\n",
    "from abc import ABC, abstractmethod\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Download required NLTK data\n",
    "def download_nltk_data():\n",
    "    \"\"\"Download required NLTK data packages\"\"\"\n",
    "    print(\"Setting up NLTK data packages...\")\n",
    "    \n",
    "    # Essential packages for text processing\n",
    "    essential_packages = [\n",
    "        (\"tokenizers/punkt\", \"punkt\"),\n",
    "        (\"corpora/stopwords\", \"stopwords\"),\n",
    "        (\"corpora/wordnet\", \"wordnet\")\n",
    "    ]\n",
    "    \n",
    "    # Additional packages that might be useful\n",
    "    additional_packages = [\"punkt_tab\"]\n",
    "    \n",
    "    # Check and download essential packages\n",
    "    for path, package in essential_packages:\n",
    "        try:\n",
    "            nltk.data.find(path)\n",
    "            print(f\"{package} already available\")\n",
    "        except LookupError:\n",
    "            try:\n",
    "                print(f\"Downloading NLTK package: {package}\")\n",
    "                nltk.download(package, quiet=True)\n",
    "                # Verify download\n",
    "                try:\n",
    "                    nltk.data.find(path)\n",
    "                    print(f\"{package} downloaded and verified\")\n",
    "                except:\n",
    "                    print(f\"{package} downloaded but verification failed\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to download {package}: {e}\")\n",
    "    \n",
    "    # Try to download additional packages (may not be available in all NLTK versions)\n",
    "    for package in additional_packages:\n",
    "        try:\n",
    "            nltk.download(package, quiet=True)\n",
    "            print(f\"{package} downloaded (if available)\")\n",
    "        except Exception as e:\n",
    "            print(f\"{package} not available: {e}\")\n",
    "    \n",
    "    print(\"NLTK data setup completed\")\n",
    "\n",
    "# Download NLTK data\n",
    "download_nltk_data()\n",
    "\n",
    "class TextProcessor(ABC):\n",
    "    \"\"\"Abstract base class for text processing steps\"\"\"\n",
    "    \n",
    "    @abstractmethod\n",
    "    def process(self, text: str) -> str:\n",
    "        \"\"\"Process a single text string\"\"\"\n",
    "        pass\n",
    "\n",
    "class Tokenizer(TextProcessor):\n",
    "    \"\"\"Tokenize text into words or sentences\"\"\"\n",
    "    \n",
    "    def __init__(self, token_type: str = 'word'):\n",
    "        \"\"\"\n",
    "            token_type: 'word' or 'sentence'\n",
    "        \"\"\"\n",
    "        self.token_type = token_type\n",
    "    \n",
    "    def process(self, text: str) -> List[str]:\n",
    "        if self.token_type == 'word':\n",
    "            return word_tokenize(text)\n",
    "        elif self.token_type == 'sentence':\n",
    "            return sent_tokenize(text)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown token_type: {self.token_type}\")\n",
    "\n",
    "class Normalizer(TextProcessor):\n",
    "    \"\"\"Normalize text (lowercase, lemmatize, stem)\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 lowercase: bool = True,\n",
    "                 lemmatize: bool = True,\n",
    "                 stem: bool = False,\n",
    "                 remove_punctuation: bool = True,\n",
    "                 remove_numbers: bool = False):\n",
    "        \"\"\"\n",
    "            lowercase: Convert to lowercase\n",
    "            lemmatize: Use lemmatization\n",
    "            stem: Use stemming (overrides lemmatize)\n",
    "            remove_punctuation: Remove punctuation\n",
    "            remove_numbers: Remove numbers\n",
    "        \"\"\"\n",
    "        self.lowercase = lowercase\n",
    "        self.lemmatize = lemmatize\n",
    "        self.stem = stem\n",
    "        self.remove_punctuation = remove_punctuation\n",
    "        self.remove_numbers = remove_numbers\n",
    "        \n",
    "        # Initialize stemmer/lemmatizer\n",
    "        if self.stem:\n",
    "            self.stemmer = PorterStemmer()\n",
    "        elif self.lemmatize:\n",
    "            self.lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    def process(self, text: str) -> str:\n",
    "        # Convert to lowercase\n",
    "        if self.lowercase:\n",
    "            text = text.lower()\n",
    "        \n",
    "        # Remove punctuation\n",
    "        if self.remove_punctuation:\n",
    "            text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "        \n",
    "        # Remove numbers\n",
    "        if self.remove_numbers:\n",
    "            text = re.sub(r'\\d+', '', text)\n",
    "        \n",
    "        # Tokenize for processing\n",
    "        tokens = word_tokenize(text)\n",
    "        \n",
    "        # Apply stemming or lemmatization\n",
    "        if self.stem:\n",
    "            tokens = [self.stemmer.stem(token) for token in tokens]\n",
    "        elif self.lemmatize:\n",
    "            tokens = [self.lemmatizer.lemmatize(token) for token in tokens]\n",
    "        \n",
    "        return ' '.join(tokens)\n",
    "\n",
    "class Filter(TextProcessor):\n",
    "    \"\"\"Filter tokens (stop words, length, etc.)\"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 remove_stopwords: bool = True,\n",
    "                 min_length: int = 2,\n",
    "                 max_length: int = 50,\n",
    "                 custom_stopwords: List[str] = None,\n",
    "                 keep_medical_terms: bool = True):\n",
    "        \"\"\"\n",
    "            remove_stopwords: Remove common stop words\n",
    "            min_length: Minimum token length\n",
    "            max_length: Maximum token length\n",
    "            custom_stopwords: Additional stop words to remove\n",
    "            keep_medical_terms: Keep medical terminology\n",
    "        \"\"\"\n",
    "        self.remove_stopwords = remove_stopwords\n",
    "        self.min_length = min_length\n",
    "        self.max_length = max_length\n",
    "        self.keep_medical_terms = keep_medical_terms\n",
    "        \n",
    "        # Load stop words\n",
    "        if self.remove_stopwords:\n",
    "            self.stop_words = set(stopwords.words('english'))\n",
    "            if custom_stopwords:\n",
    "                self.stop_words.update(custom_stopwords)\n",
    "        \n",
    "        # Medical terms to preserve\n",
    "        if self.keep_medical_terms:\n",
    "            self.medical_terms = {\n",
    "                'atelectasis', 'pneumothorax', 'effusion', 'consolidation', 'edema',\n",
    "                'cardiomegaly', 'pneumonia', 'infiltrate', 'opacity', 'pleural',\n",
    "                'mediastinal', 'hilar', 'hemidiaphragm', 'bronchitis', 'fibrosis',\n",
    "                'nodule', 'mass', 'lesion', 'fracture', 'displacement', 'enlargement',\n",
    "                'clear', 'normal', 'abnormal', 'acute', 'chronic', 'bilateral',\n",
    "                'unilateral', 'focal', 'diffuse', 'segmental', 'lobar'\n",
    "            }\n",
    "    \n",
    "    def process(self, text: str) -> str:\n",
    "        tokens = word_tokenize(text)\n",
    "        filtered_tokens = []\n",
    "        \n",
    "        for token in tokens:\n",
    "            # Length filtering\n",
    "            if len(token) < self.min_length or len(token) > self.max_length:\n",
    "                continue\n",
    "            \n",
    "            # Stop word filtering\n",
    "            if self.remove_stopwords and token in self.stop_words:\n",
    "                # Keep medical terms even if they're stop words\n",
    "                if self.keep_medical_terms and token in self.medical_terms:\n",
    "                    filtered_tokens.append(token)\n",
    "                continue\n",
    "            \n",
    "            filtered_tokens.append(token)\n",
    "        \n",
    "        return ' '.join(filtered_tokens)\n",
    "\n",
    "class TextProcessingPipeline:\n",
    "    \"\"\"Main text processing pipeline\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Dict[str, Any] = None):\n",
    "        \"\"\"\n",
    "        Initialize pipeline with configuration\n",
    "        \n",
    "        Args:\n",
    "            config: Configuration dictionary\n",
    "        \"\"\"\n",
    "        self.config = config or self.get_default_config()\n",
    "        self.processors = []\n",
    "        self.setup_pipeline()\n",
    "    \n",
    "    def get_default_config(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get default configuration\"\"\"\n",
    "        return {\n",
    "            'tokenizer': {\n",
    "                'token_type': 'word'\n",
    "            },\n",
    "            'normalizer': {\n",
    "                'lowercase': True,\n",
    "                'lemmatize': True,\n",
    "                'stem': False,\n",
    "                'remove_punctuation': True,\n",
    "                'remove_numbers': False\n",
    "            },\n",
    "            'filter': {\n",
    "                'remove_stopwords': True,\n",
    "                'min_length': 2,\n",
    "                'max_length': 50,\n",
    "                'custom_stopwords': ['patient', 'study', 'examination', 'technique'],\n",
    "                'keep_medical_terms': True\n",
    "            },\n",
    "            'output': {\n",
    "                'save_processed': True,\n",
    "                'save_statistics': True,\n",
    "                'output_dir': './data/processed_text'\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def setup_pipeline(self):\n",
    "        \"\"\"Setup processing pipeline based on configuration\"\"\"\n",
    "        # Tokenizer\n",
    "        if 'tokenizer' in self.config:\n",
    "            self.processors.append(Tokenizer(**self.config['tokenizer']))\n",
    "        \n",
    "        # Normalizer\n",
    "        if 'normalizer' in self.config:\n",
    "            self.processors.append(Normalizer(**self.config['normalizer']))\n",
    "        \n",
    "        # Filter\n",
    "        if 'filter' in self.config:\n",
    "            self.processors.append(Filter(**self.config['filter']))\n",
    "    \n",
    "    def process_text(self, text: str) -> str:\n",
    "        \"\"\"Process a single text through the pipeline\"\"\"\n",
    "        if not text or pd.isna(text):\n",
    "            return \"\"\n",
    "        \n",
    "        result = text\n",
    "        for processor in self.processors:\n",
    "            result = processor.process(result)\n",
    "            # Ensure result is a string\n",
    "            if isinstance(result, list):\n",
    "                result = ' '.join(result)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def process_captions(self, captions: List[str]) -> List[str]:\n",
    "        \"\"\"Process a list of captions\"\"\"\n",
    "        processed_captions = []\n",
    "        for caption in captions:\n",
    "            if isinstance(caption, str):\n",
    "                processed = self.process_text(caption)\n",
    "                processed_captions.append(processed)\n",
    "            else:\n",
    "                processed_captions.append(\"\")\n",
    "        return processed_captions\n",
    "\n",
    "class MIMICCaptionProcessor:\n",
    "    \"\"\"Main class for processing MIMIC-CXR captions\"\"\"\n",
    "    \n",
    "    def __init__(self, csv_path: str, config: Dict[str, Any] = None):\n",
    "        \"\"\"\n",
    "        Initialize processor\n",
    "        \n",
    "        Args:\n",
    "            csv_path: Path to the CSV file with sampled data\n",
    "            config: Processing configuration\n",
    "        \"\"\"\n",
    "        self.csv_path = csv_path\n",
    "        self.config = config\n",
    "        self.pipeline = TextProcessingPipeline(config)\n",
    "        self.df = None\n",
    "        self.processed_data = []\n",
    "        self.statistics = {}\n",
    "        \n",
    "    def load_data(self):\n",
    "        \"\"\"Load data from CSV file\"\"\"\n",
    "        print(\"Loading data from CSV...\")\n",
    "        self.df = pd.read_csv(self.csv_path)\n",
    "        print(f\"Loaded {len(self.df)} samples\")\n",
    "    \n",
    "    def extract_captions(self):\n",
    "        \"\"\"Extract captions from UMLS JSON data\"\"\"\n",
    "        print(\"Extracting captions from UMLS data...\")\n",
    "        all_captions = []\n",
    "        \n",
    "        for idx, row in self.df.iterrows():\n",
    "            try:\n",
    "                umls_data = json.loads(row['umls_json_info'])\n",
    "                if 'caption' in umls_data:\n",
    "                    captions = umls_data['caption']\n",
    "                    if isinstance(captions, list):\n",
    "                        # Join multiple captions\n",
    "                        caption_text = ' '.join(captions)\n",
    "                    else:\n",
    "                        caption_text = str(captions)\n",
    "                    all_captions.append(caption_text)\n",
    "                else:\n",
    "                    all_captions.append(\"\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing row {idx}: {e}\")\n",
    "                all_captions.append(\"\")\n",
    "        \n",
    "        self.df['raw_captions'] = all_captions\n",
    "        print(f\"Extracted captions from {len(all_captions)} samples\")\n",
    "    \n",
    "    def process_captions(self):\n",
    "        \"\"\"Process all captions through the pipeline\"\"\"\n",
    "        print(\"Processing captions through pipeline...\")\n",
    "        processed_captions = []\n",
    "        \n",
    "        for idx, caption in enumerate(self.df['raw_captions']):\n",
    "            if idx % 100 == 0:\n",
    "                print(f\"  Processing sample {idx}/{len(self.df)}\")\n",
    "            \n",
    "            processed = self.pipeline.process_text(caption)\n",
    "            processed_captions.append(processed)\n",
    "        \n",
    "        self.df['processed_captions'] = processed_captions\n",
    "        print(\"Caption processing completed\")\n",
    "    \n",
    "    def calculate_statistics(self):\n",
    "        \"\"\"Calculate processing statistics\"\"\"\n",
    "        print(\"Calculating processing statistics...\")\n",
    "        \n",
    "        # Basic statistics\n",
    "        total_samples = len(self.df)\n",
    "        non_empty_raw = sum(1 for x in self.df['raw_captions'] if x.strip())\n",
    "        non_empty_processed = sum(1 for x in self.df['processed_captions'] if x.strip())\n",
    "        \n",
    "        # Length statistics\n",
    "        raw_lengths = [len(x.split()) for x in self.df['raw_captions'] if x.strip()]\n",
    "        processed_lengths = [len(x.split()) for x in self.df['processed_captions'] if x.strip()]\n",
    "        \n",
    "        # Vocabulary statistics\n",
    "        all_processed_text = ' '.join(self.df['processed_captions'].tolist())\n",
    "        all_tokens = all_processed_text.split()\n",
    "        unique_tokens = set(all_tokens)\n",
    "        \n",
    "        self.statistics = {\n",
    "            'total_samples': total_samples,\n",
    "            'non_empty_raw': non_empty_raw,\n",
    "            'non_empty_processed': non_empty_processed,\n",
    "            'processing_retention_rate': non_empty_processed / non_empty_raw if non_empty_raw > 0 else 0,\n",
    "            'raw_length_stats': {\n",
    "                'mean': np.mean(raw_lengths) if raw_lengths else 0,\n",
    "                'median': np.median(raw_lengths) if raw_lengths else 0,\n",
    "                'min': np.min(raw_lengths) if raw_lengths else 0,\n",
    "                'max': np.max(raw_lengths) if raw_lengths else 0\n",
    "            },\n",
    "            'processed_length_stats': {\n",
    "                'mean': np.mean(processed_lengths) if processed_lengths else 0,\n",
    "                'median': np.median(processed_lengths) if processed_lengths else 0,\n",
    "                'min': np.min(processed_lengths) if processed_lengths else 0,\n",
    "                'max': np.max(processed_lengths) if processed_lengths else 0\n",
    "            },\n",
    "            'vocabulary_size': len(unique_tokens),\n",
    "            'total_tokens': len(all_tokens),\n",
    "            'most_common_tokens': Counter(all_tokens).most_common(20)\n",
    "        }\n",
    "        \n",
    "        print(\"Statistics calculated\")\n",
    "    \n",
    "    def print_statistics(self):\n",
    "        \"\"\"Print processing statistics\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"TEXT PROCESSING STATISTICS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        stats = self.statistics\n",
    "        \n",
    "        print(f\"Dataset Overview:\")\n",
    "        print(f\"  - Total samples: {stats['total_samples']}\")\n",
    "        print(f\"  - Non-empty raw captions: {stats['non_empty_raw']}\")\n",
    "        print(f\"  - Non-empty processed captions: {stats['non_empty_processed']}\")\n",
    "        print(f\"  - Processing retention rate: {stats['processing_retention_rate']:.1%}\")\n",
    "        \n",
    "        print(f\"\\nLength Statistics:\")\n",
    "        print(f\"  Raw captions:\")\n",
    "        raw_stats = stats['raw_length_stats']\n",
    "        print(f\"    - Mean: {raw_stats['mean']:.1f} words\")\n",
    "        print(f\"    - Median: {raw_stats['median']:.1f} words\")\n",
    "        print(f\"    - Min: {raw_stats['min']} words\")\n",
    "        print(f\"    - Max: {raw_stats['max']} words\")\n",
    "        \n",
    "        print(f\"  Processed captions:\")\n",
    "        proc_stats = stats['processed_length_stats']\n",
    "        print(f\"    - Mean: {proc_stats['mean']:.1f} words\")\n",
    "        print(f\"    - Median: {proc_stats['median']:.1f} words\")\n",
    "        print(f\"    - Min: {proc_stats['min']} words\")\n",
    "        print(f\"    - Max: {proc_stats['max']} words\")\n",
    "        \n",
    "        print(f\"\\nVocabulary Statistics:\")\n",
    "        print(f\"  - Unique tokens: {stats['vocabulary_size']}\")\n",
    "        print(f\"  - Total tokens: {stats['total_tokens']}\")\n",
    "        print(f\"  - Average tokens per caption: {stats['total_tokens'] / stats['non_empty_processed']:.1f}\")\n",
    "        \n",
    "        print(f\"\\nTop 20 Most Common Tokens:\")\n",
    "        for token, count in stats['most_common_tokens']:\n",
    "            print(f\"  - '{token}': {count} times\")\n",
    "    \n",
    "    def save_results(self):\n",
    "        \"\"\"Save processed data and statistics\"\"\"\n",
    "        output_dir = Path(self.config.get('output', {}).get('output_dir', './data/processed_text'))\n",
    "        output_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Save processed data\n",
    "        if self.config.get('output', {}).get('save_processed', True):\n",
    "            output_file = output_dir / 'processed_captions.csv'\n",
    "            self.df[['index', 'original_index', 'raw_captions', 'processed_captions']].to_csv(\n",
    "                output_file, index=False\n",
    "            )\n",
    "            print(f\"Processed data saved to: {output_file}\")\n",
    "        \n",
    "        # Save statistics\n",
    "        if self.config.get('output', {}).get('save_statistics', True):\n",
    "            stats_file = output_dir / 'processing_statistics.json'\n",
    "            # Convert numpy types to Python types for JSON serialization\n",
    "            serializable_stats = self._make_serializable(self.statistics)\n",
    "            with open(stats_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump(serializable_stats, f, indent=2, ensure_ascii=False)\n",
    "            print(f\"Statistics saved to: {stats_file}\")\n",
    "    \n",
    "    def _make_serializable(self, obj):\n",
    "        \"\"\"Convert numpy types to Python types for JSON serialization\"\"\"\n",
    "        if isinstance(obj, dict):\n",
    "            return {key: self._make_serializable(value) for key, value in obj.items()}\n",
    "        elif isinstance(obj, list):\n",
    "            return [self._make_serializable(item) for item in obj]\n",
    "        elif isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        else:\n",
    "            return obj\n",
    "        \n",
    "        # Save configuration (unreachable from here; kept if you refactor)\n",
    "        config_file = output_dir / 'processing_config.json'\n",
    "        with open(config_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(self.config, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"Configuration saved to: {config_file}\")\n",
    "    \n",
    "    def run_full_pipeline(self):\n",
    "        \"\"\"Run the complete text processing pipeline\"\"\"\n",
    "        print(\"Starting MIMIC-CXR Caption Processing Pipeline\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Load data\n",
    "        self.load_data()\n",
    "        \n",
    "        # Extract captions\n",
    "        self.extract_captions()\n",
    "        \n",
    "        # Process captions\n",
    "        self.process_captions()\n",
    "        \n",
    "        # Calculate statistics\n",
    "        self.calculate_statistics()\n",
    "        \n",
    "        # Print statistics\n",
    "        self.print_statistics()\n",
    "        \n",
    "        # Save results\n",
    "        self.save_results()\n",
    "        \n",
    "        print(\"\\nText processing pipeline completed successfully!\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the text processing pipeline\"\"\"\n",
    "    # Configuration for medical text processing\n",
    "    config = {\n",
    "        'tokenizer': {\n",
    "            'token_type': 'word'\n",
    "        },\n",
    "        'normalizer': {\n",
    "            'lowercase': True,\n",
    "            'lemmatize': True,\n",
    "            'stem': False,\n",
    "            'remove_punctuation': True,\n",
    "            'remove_numbers': False\n",
    "        },\n",
    "        'filter': {\n",
    "            'remove_stopwords': True,\n",
    "            'min_length': 2,\n",
    "            'max_length': 50,\n",
    "            'custom_stopwords': [\n",
    "                'patient', 'study', 'examination', 'technique', 'comparison',\n",
    "                'findings', 'impression', 'report', 'history', 'indication'\n",
    "            ],\n",
    "            'keep_medical_terms': True\n",
    "        },\n",
    "        'output': {\n",
    "            'save_processed': True,\n",
    "            'save_statistics': True,\n",
    "            'output_dir': './data/processed_text'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Initialize processor\n",
    "    csv_path = './data/sampled_1000_data.csv'\n",
    "    processor = MIMICCaptionProcessor(csv_path, config)\n",
    "    \n",
    "    # Run pipeline\n",
    "    processor.run_full_pipeline()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1e5fbd-ebef-4df0-b192-8e3d6c4bdd8a",
   "metadata": {},
   "source": [
    "##  Text Clustering\n",
    "\n",
    "In this part, we conduct the UMAP clustering analysis as well as the tfidf clustering. We have classification label RADGRAPH_METRIC_LABELS (75 classes used) and we do mapping between this label and clusters, there are some connections. But as it is a multi-label classification task so the inter-class disparity is not that clear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6a7cdbc7-580f-4692-8755-6cdb41545cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting MIMIC-CXR Caption TF-IDF Clustering Analysis\n",
      "============================================================\n",
      "Loading data...\n",
      "Loaded 1000 original samples\n",
      "Loaded 1000 processed samples\n",
      "Preparing ground-truth labels and resolving names...\n",
      "Resolved 75 classes. Names saved to results/debug_label_names.txt\n",
      "Creating TF-IDF vectors...\n",
      "TF-IDF matrix: (1000, 1000) (docs × feats)\n",
      "Vocabulary size: 1000\n",
      "\n",
      "Top 20 TF-IDF features:\n",
      "   1. right                (score: 0.0534)\n",
      "   2. lung                 (score: 0.0500)\n",
      "   3. pleural              (score: 0.0469)\n",
      "   4. left                 (score: 0.0461)\n",
      "   5. effusion             (score: 0.0459)\n",
      "   6. normal               (score: 0.0459)\n",
      "   7. pleural effusion     (score: 0.0435)\n",
      "   8. acute                (score: 0.0397)\n",
      "   9. pneumothorax         (score: 0.0395)\n",
      "  10. pulmonary            (score: 0.0395)\n",
      "  11. seen                 (score: 0.0336)\n",
      "  12. chest                (score: 0.0325)\n",
      "  13. silhouette           (score: 0.0323)\n",
      "  14. consolidation        (score: 0.0321)\n",
      "  15. clear                (score: 0.0319)\n",
      "  16. effusion pneumothorax (score: 0.0319)\n",
      "  17. atelectasis          (score: 0.0308)\n",
      "  18. focal                (score: 0.0304)\n",
      "  19. edema                (score: 0.0302)\n",
      "  20. unchanged            (score: 0.0300)\n",
      "Clustering with K values: [3, 4, 5, 6, 7, 8]\n",
      "\n",
      "Clustering with K=3...\n",
      "  K=3: Silhouette=0.040, CH=47.9, DB=3.222\n",
      "\n",
      "Clustering with K=4...\n",
      "  K=4: Silhouette=0.042, CH=39.1, DB=3.758\n",
      "\n",
      "Clustering with K=5...\n",
      "  K=5: Silhouette=0.040, CH=35.1, DB=4.079\n",
      "\n",
      "Clustering with K=6...\n",
      "  K=6: Silhouette=0.046, CH=32.4, DB=3.868\n",
      "\n",
      "Clustering with K=7...\n",
      "  K=7: Silhouette=0.049, CH=30.7, DB=3.596\n",
      "\n",
      "Clustering with K=8...\n",
      "  K=8: Silhouette=0.052, CH=28.8, DB=3.634\n",
      "Creating visualizations...\n",
      "Saved clustering quality plot: results/clustering_quality.png\n",
      "Creating UMAP visualization (cluster ↔ label correspondence)...\n",
      "Saved: results/umap_clusters.png\n",
      "Saved: results/umap_labels.png\n",
      "Saved: results/umap_cluster_label_match.png\n",
      "Creating label visualizations...\n",
      "Saved label analysis: results/label_analysis.png\n",
      "Generating analysis report...\n",
      "Saved analysis report: results/cluster_analysis_report.txt\n",
      "\n",
      "============================================================\n",
      "ANALYSIS SUMMARY\n",
      "============================================================\n",
      "Dataset: 1000 captions\n",
      "TF-IDF: 1000 docs × 1000 feats\n",
      "Best K (Silhouette): 8 (score: 0.052)\n",
      "Results saved to directory: results\n",
      "\n",
      "Analysis completed successfully!\n",
      "Results saved to directory: results\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import (\n",
    "    silhouette_score,\n",
    "    calinski_harabasz_score,\n",
    "    davies_bouldin_score\n",
    ")\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Optional UMAP\n",
    "try:\n",
    "    import umap\n",
    "    UMAP_AVAILABLE = True\n",
    "except Exception:\n",
    "    UMAP_AVAILABLE = False\n",
    "    print(\"UMAP not available. Install with: pip install umap-learn\")\n",
    "\n",
    "# Plot style\n",
    "try:\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "except Exception:\n",
    "    try:\n",
    "        plt.style.use('seaborn')\n",
    "    except Exception:\n",
    "        plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "from matplotlib.lines import Line2D\n",
    "try:\n",
    "    from scipy.spatial import ConvexHull\n",
    "    _HAVE_SCIPY = True\n",
    "except Exception:\n",
    "    _HAVE_SCIPY = False\n",
    "\n",
    "\n",
    "FG_RADGRAPH_LABELS = [\n",
    "    \"normal\",\"pleural effusion\",\"opacity\",\"pneumothorax\",\"edema\",\"atelectasis\",\"tube\",\n",
    "    \"consolidation\",\"enlarged cardiomediastinum\",\"tip\",\"pneumonia\",\"line\",\"cardiomegaly\",\n",
    "    \"fracture\",\"calcification\",\"device\",\"engorgement\",\"nodule\",\"wire\",\"pacemaker\",\n",
    "    \"pleural thicken\",\"marking\",\"scar\",\"hyperinflate\",\"blunt\",\"collapse\",\"emphysema\",\n",
    "    \"aerate\",\"mass\",\"infiltration\",\"obscure\",\"deformity\",\"hernia\",\"drainage\",\n",
    "    \"distention\",\"shift\",\"stent\",\"lesion\",\"hardware\",\"dilation\",\"aspiration\"\n",
    "]\n",
    "\n",
    "RADGRAPH_METRIC_LABELS = [\n",
    "    \"normal\",\"clear\",\"sharp\",\"sharply\",\"unremarkable\",\"intact\",\"stable\",\"free\",\"effusion\",\n",
    "    \"opacity\",\"pneumothorax\",\"edema\",\"atelectasis\",\"tube\",\"consolidation\",\"process\",\n",
    "    \"abnormality\",\"enlarge\",\"tip\",\"low\",\"pneumonia\",\"line\",\"congestion\",\"catheter\",\n",
    "    \"cardiomegaly\",\"fracture\",\"air\",\"tortuous\",\"lead\",\"disease\",\"calcification\",\n",
    "    \"prominence\",\"device\",\"engorgement\",\"picc\",\"clip\",\"elevation\",\"expand\",\"nodule\",\n",
    "    \"wire\",\"fluid\",\"degenerative\",\"pacemaker\",\"thicken\",\"marking\",\"scar\",\"hyperinflate\",\n",
    "    \"blunt\",\"loss\",\"widen\",\"collapse\",\"density\",\"emphysema\",\"aerate\",\"mass\",\"crowd\",\n",
    "    \"infiltrate\",\"obscure\",\"deformity\",\"hernia\",\"drainage\",\"distention\",\"shift\",\"stent\",\n",
    "    \"pressure\",\"lesion\",\"finding\",\"borderline\",\"hardware\",\"dilation\",\"chf\",\n",
    "    \"redistribution\",\"aspiration\",\"tail_abnorm_obs\",\"excluded_obs\"\n",
    "]\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "def _parse_class_list_any(obj):\n",
    "    \"\"\"\n",
    "    Parse class_list into (labels, class_names)\n",
    "    - labels: list[int] of 0/1\n",
    "    - class_names: list[str] or None when unnamed list input\n",
    "\n",
    "    Accepts: str (JSON), dict{name: val}, list[val]\n",
    "    \"\"\"\n",
    "    if isinstance(obj, str):\n",
    "        try:\n",
    "            obj = json.loads(obj)\n",
    "        except Exception:\n",
    "            return [], None\n",
    "\n",
    "    if isinstance(obj, dict):\n",
    "        # 保持出现顺序（Python3.7+默认字典有序）\n",
    "        keys = list(obj.keys())\n",
    "        vals = [obj[k] for k in keys]\n",
    "        labels = [1 if int(round(float(v))) == 1 else 0 for v in vals]\n",
    "        return labels, keys\n",
    "\n",
    "    if isinstance(obj, list):\n",
    "        try:\n",
    "            labels = [1 if int(round(float(v))) == 1 else 0 for v in obj]\n",
    "        except Exception:\n",
    "            labels = [1 if v == 1 else 0 for v in obj]\n",
    "        return labels, None\n",
    "\n",
    "    return [], None\n",
    "\n",
    "\n",
    "class MIMICCaptionClusterer:\n",
    "    \"\"\"TF-IDF clustering analysis for MIMIC-CXR captions with named labels\"\"\"\n",
    "\n",
    "    def __init__(self, csv_path: str, processed_csv_path: str = None, label_scheme: str = \"auto\"):\n",
    "        self.csv_path = csv_path\n",
    "        self.processed_csv_path = processed_csv_path or './data/processed_text/processed_captions.csv'\n",
    "        self.label_scheme = label_scheme  # 'auto' | 'radgraph_metric' | 'fg_radgraph'\n",
    "\n",
    "        self.df_original: pd.DataFrame = None\n",
    "        self.df_processed: pd.DataFrame = None\n",
    "        self.vectorizer: TfidfVectorizer = None\n",
    "        self.X_tfidf = None\n",
    "        self.results: pd.DataFrame = pd.DataFrame()\n",
    "        self.num_classes: int = 0\n",
    "        self.class_names: List[str] = []\n",
    "\n",
    "    # ---------- IO ----------\n",
    "    def load_data(self):\n",
    "        print(\"Loading data...\")\n",
    "        self.df_original = pd.read_csv(self.csv_path)\n",
    "        print(f\"Loaded {len(self.df_original)} original samples\")\n",
    "\n",
    "        if Path(self.processed_csv_path).exists():\n",
    "            self.df_processed = pd.read_csv(self.processed_csv_path)\n",
    "            print(f\"Loaded {len(self.df_processed)} processed samples\")\n",
    "            if 'processed_captions' not in self.df_processed.columns:\n",
    "                print(\"processed_captions not found in processed CSV; extracting from original\")\n",
    "                self.df_processed = self.df_original.copy()\n",
    "                self._extract_captions_from_original()\n",
    "        else:\n",
    "            print(\"Processed data not found; extracting captions from original\")\n",
    "            self.df_processed = self.df_original.copy()\n",
    "            self._extract_captions_from_original()\n",
    "\n",
    "        if 'processed_captions' not in self.df_processed.columns:\n",
    "            raise ValueError(\"processed_captions column is required or must be derivable from original UMLS JSON.\")\n",
    "\n",
    "    def _extract_captions_from_original(self):\n",
    "        print(\"Extracting captions from original data...\")\n",
    "        captions = []\n",
    "        for idx, row in self.df_original.iterrows():\n",
    "            try:\n",
    "                umls_data = json.loads(row['umls_json_info'])\n",
    "                if 'caption' in umls_data:\n",
    "                    cap = umls_data['caption']\n",
    "                    if isinstance(cap, list):\n",
    "                        caption_text = ' '.join(map(str, cap))\n",
    "                    else:\n",
    "                        caption_text = str(cap)\n",
    "                    captions.append(caption_text)\n",
    "                else:\n",
    "                    captions.append(\"\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing row {idx}: {e}\")\n",
    "                captions.append(\"\")\n",
    "        self.df_processed['raw_captions'] = captions\n",
    "        self.df_processed['processed_captions'] = captions\n",
    "        print(f\"Extracted {len(captions)} captions\")\n",
    "\n",
    "    # ---------- Label naming helpers ----------\n",
    "    def _force_length(self, names: List[str], L: int) -> List[str]:\n",
    "        \"\"\"Return list of length L: truncate or pad with label_i.\"\"\"\n",
    "        if L <= len(names):\n",
    "            return names[:L]\n",
    "        return names + [f\"label_{i}\" for i in range(len(names), L)]\n",
    "\n",
    "    def _choose_class_names(self, length: int) -> List[str]:\n",
    "        \"\"\"Choose class names by scheme or auto-match length. Never returns class_xx.\"\"\"\n",
    "        if self.label_scheme == \"radgraph_metric\":\n",
    "            return self._force_length(RADGRAPH_METRIC_LABELS, length)\n",
    "        if self.label_scheme == \"fg_radgraph\":\n",
    "            return self._force_length(FG_RADGRAPH_LABELS, length)\n",
    "\n",
    "        # auto\n",
    "        if length == len(RADGRAPH_METRIC_LABELS):\n",
    "            return RADGRAPH_METRIC_LABELS.copy()\n",
    "        if length == len(FG_RADGRAPH_LABELS):\n",
    "            return FG_RADGRAPH_LABELS.copy()\n",
    "        return [f\"label_{i}\" for i in range(length)]\n",
    "\n",
    "    # ---------- Labels ----------\n",
    "    def prepare_labels(self):\n",
    "        print(\"Preparing ground-truth labels and resolving names...\")\n",
    "        parsed = []\n",
    "        max_len = 0\n",
    "        has_named_dict = False\n",
    "\n",
    "        for _, row in self.df_original.iterrows():\n",
    "            labels, names = _parse_class_list_any(row.get('class_list', []))\n",
    "            parsed.append((labels, names))\n",
    "            max_len = max(max_len, len(labels))\n",
    "            if names is not None:\n",
    "                has_named_dict = True\n",
    "\n",
    "        # Decide final class_names\n",
    "        if self.label_scheme != \"auto\":\n",
    "            # Force our mapping regardless of original names\n",
    "            C = max_len if max_len > 0 else 1\n",
    "            class_names = self._choose_class_names(C)\n",
    "            print(f\"Using label scheme: {self.label_scheme} (num_classes={len(class_names)})\")\n",
    "            name_index = None\n",
    "        else:\n",
    "            # auto\n",
    "            if has_named_dict:\n",
    "                # Use the union of keys in their first-seen order\n",
    "                seen = []\n",
    "                for _, names in parsed:\n",
    "                    if names is None:\n",
    "                        continue\n",
    "                    for n in names:\n",
    "                        if n not in seen:\n",
    "                            seen.append(n)\n",
    "                class_names = seen\n",
    "                C = len(class_names)\n",
    "                name_index = {n: i for i, n in enumerate(class_names)}\n",
    "            else:\n",
    "                C = max_len if max_len > 0 else 1\n",
    "                class_names = self._choose_class_names(C)\n",
    "                name_index = None\n",
    "\n",
    "        # Build unified 0/1 vectors aligned to C\n",
    "        unified_labels = []\n",
    "        for (lbls, names) in parsed:\n",
    "            vec = [0] * C\n",
    "            if (names is not None) and (self.label_scheme == \"auto\") and has_named_dict:\n",
    "                idx_map = {n: i for i, n in enumerate(class_names)}\n",
    "                for n, v in zip(names, lbls):\n",
    "                    if n in idx_map and idx_map[n] < C:\n",
    "                        vec[idx_map[n]] = 1 if v == 1 else 0\n",
    "            else:\n",
    "                for i, v in enumerate(lbls[:C]):\n",
    "                    vec[i] = 1 if v == 1 else 0\n",
    "            unified_labels.append(vec)\n",
    "\n",
    "        self.df_processed['binary_labels'] = unified_labels\n",
    "        self.df_processed['label_names'] = [class_names] * len(self.df_processed)\n",
    "        self.num_classes = C\n",
    "        self.class_names = class_names\n",
    "\n",
    "        # Dump the final class_names for audit\n",
    "        Path(\"results\").mkdir(exist_ok=True)\n",
    "        with open(\"results/debug_label_names.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "            for i, n in enumerate(self.class_names):\n",
    "                f.write(f\"{i}\\t{n}\\n\")\n",
    "        print(f\"Resolved {self.num_classes} classes. Names saved to results/debug_label_names.txt\")\n",
    "\n",
    "    # ---------- Text ----------\n",
    "    def vectorize_text(self, min_df: int = 2, max_features: int = 1000, ngram_range: Tuple[int, int] = (1, 2)):\n",
    "        print(\"Creating TF-IDF vectors...\")\n",
    "        texts = self.df_processed['processed_captions'].fillna('').astype(str).tolist()\n",
    "\n",
    "        self.vectorizer = TfidfVectorizer(\n",
    "            stop_words='english',\n",
    "            ngram_range=ngram_range,\n",
    "            min_df=min_df,\n",
    "            max_features=max_features,\n",
    "            lowercase=True,\n",
    "            strip_accents='unicode'\n",
    "        )\n",
    "        self.X_tfidf = self.vectorizer.fit_transform(texts)\n",
    "        print(f\"TF-IDF matrix: {self.X_tfidf.shape} (docs × feats)\")\n",
    "        print(f\"Vocabulary size: {len(self.vectorizer.vocabulary_)}\")\n",
    "\n",
    "        # Show top 20 features by mean TF-IDF\n",
    "        feature_names = self.vectorizer.get_feature_names_out()\n",
    "        tfidf_scores = np.asarray(self.X_tfidf.mean(axis=0)).flatten()\n",
    "        top_idx = np.argsort(tfidf_scores)[-20:][::-1]\n",
    "        print(\"\\nTop 20 TF-IDF features:\")\n",
    "        for i, j in enumerate(top_idx, start=1):\n",
    "            print(f\"  {i:2d}. {feature_names[j]:<20} (score: {tfidf_scores[j]:.4f})\")\n",
    "\n",
    "    # ---------- Clustering ----------\n",
    "    def cluster_and_evaluate(self, ks: List[int], random_state: int = 42):\n",
    "        print(f\"Clustering with K values: {ks}\")\n",
    "        results = []\n",
    "\n",
    "        svd_dim = min(100, self.X_tfidf.shape[1] - 1) if self.X_tfidf.shape[1] > 1 else 1\n",
    "        svd = TruncatedSVD(n_components=svd_dim, random_state=random_state)\n",
    "        X_reduced = svd.fit_transform(self.X_tfidf)\n",
    "\n",
    "        for K in ks:\n",
    "            print(f\"\\nClustering with K={K}...\")\n",
    "            kmeans = KMeans(\n",
    "                n_clusters=K,\n",
    "                n_init=10,\n",
    "                max_iter=300,\n",
    "                random_state=random_state,\n",
    "                algorithm='lloyd'\n",
    "            )\n",
    "            cluster_labels = kmeans.fit_predict(self.X_tfidf)\n",
    "\n",
    "            silhouette = silhouette_score(self.X_tfidf, cluster_labels, metric='cosine')\n",
    "            calinski_harabasz = calinski_harabasz_score(X_reduced, cluster_labels)\n",
    "            davies_bouldin = davies_bouldin_score(X_reduced, cluster_labels)\n",
    "\n",
    "            results.append({\n",
    "                'K': K,\n",
    "                'silhouette_score': float(silhouette),\n",
    "                'calinski_harabasz_score': float(calinski_harabasz),\n",
    "                'davies_bouldin_score': float(davies_bouldin),\n",
    "                'inertia': float(kmeans.inertia_)\n",
    "            })\n",
    "\n",
    "            # Per-sample export with soft probabilities\n",
    "            df_out = self.df_processed.copy()\n",
    "            df_out['predicted_cluster'] = cluster_labels\n",
    "            dists = kmeans.transform(self.X_tfidf)  # (N, K)\n",
    "            probs = np.exp(-dists)\n",
    "            probs /= probs.sum(axis=1, keepdims=True)\n",
    "            df_out['cluster_prob'] = probs.max(axis=1)\n",
    "\n",
    "            def dominant_label_idx(bl):\n",
    "                arr = np.array(bl, dtype=int)\n",
    "                return int(arr.argmax()) if arr.sum() > 0 else -1\n",
    "\n",
    "            dom_idx = [dominant_label_idx(bl) for bl in df_out['binary_labels']]\n",
    "            dom_name = [self.class_names[i] if (isinstance(i, (int, np.integer)) and 0 <= i < self.num_classes) else 'none'\n",
    "                        for i in dom_idx]\n",
    "            df_out['dominant_label'] = dom_name\n",
    "\n",
    "            Path('results').mkdir(exist_ok=True)\n",
    "            df_out[['processed_captions', 'dominant_label', 'predicted_cluster', 'cluster_prob']].to_csv(\n",
    "                f'results/caption_clusters_K{K}.csv', index=False\n",
    "            )\n",
    "\n",
    "            self._analyze_cluster_labels(K, cluster_labels)\n",
    "            print(f\"  K={K}: Silhouette={silhouette:.3f}, CH={calinski_harabasz:.1f}, DB={davies_bouldin:.3f}\")\n",
    "\n",
    "        self.results = pd.DataFrame(results)\n",
    "        return self.results\n",
    "\n",
    "    def _analyze_cluster_labels(self, K: int, cluster_labels: np.ndarray):\n",
    "        Path('results').mkdir(exist_ok=True)\n",
    "        df_analysis = self.df_processed.copy()\n",
    "        df_analysis['cluster'] = cluster_labels\n",
    "\n",
    "        rows = []\n",
    "        for cluster_id in range(K):\n",
    "            mask = (df_analysis['cluster'] == cluster_id)\n",
    "            cluster_samples = df_analysis[mask]\n",
    "            if len(cluster_samples) == 0:\n",
    "                continue\n",
    "\n",
    "            cls_arr = np.array([lbl for lbl in cluster_samples['binary_labels']], dtype=int)\n",
    "            counts = cls_arr.sum(axis=0) if cls_arr.size > 0 else np.zeros(self.num_classes, dtype=int)\n",
    "            top = sorted([(self.class_names[i], int(counts[i])) for i in range(self.num_classes)],\n",
    "                         key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "            rows.append({\n",
    "                'cluster': cluster_id,\n",
    "                'size': int(mask.sum()),\n",
    "                'top_classes': json.dumps(top, ensure_ascii=False)\n",
    "            })\n",
    "\n",
    "        pd.DataFrame(rows).to_csv(f'results/cluster_analysis_K{K}.csv', index=False)\n",
    "        self._create_cluster_label_matrix(K, cluster_labels)\n",
    "\n",
    "    def _create_cluster_label_matrix(self, K: int, cluster_labels: np.ndarray):\n",
    "        all_labels = np.array([lbl for lbl in self.df_processed['binary_labels']], dtype=int)\n",
    "        class_counts = all_labels.sum(axis=0) if all_labels.size > 0 else np.zeros(self.num_classes, dtype=int)\n",
    "        top_k = min(10, self.num_classes)\n",
    "        top_class_indices = np.argsort(class_counts)[-top_k:][::-1]\n",
    "\n",
    "        rows = []\n",
    "        for cluster_id in range(K):\n",
    "            mask = (cluster_labels == cluster_id)\n",
    "            cluster_samples = self.df_processed[mask]\n",
    "            row = {'cluster': cluster_id, 'size': int(mask.sum())}\n",
    "            if len(cluster_samples) > 0:\n",
    "                cls_arr = np.array([lbl for lbl in cluster_samples['binary_labels']], dtype=int)\n",
    "                for class_idx in top_class_indices:\n",
    "                    cname = self.class_names[class_idx]\n",
    "                    row[cname] = int(cls_arr[:, class_idx].sum()) if cls_arr.size > 0 else 0\n",
    "            else:\n",
    "                for class_idx in top_class_indices:\n",
    "                    cname = self.class_names[class_idx]\n",
    "                    row[cname] = 0\n",
    "            rows.append(row)\n",
    "\n",
    "        pd.DataFrame(rows).to_csv(f'results/cluster_vs_labels_K{K}.csv', index=False)\n",
    "\n",
    "    # ---------- Visualization ----------\n",
    "    def create_visualizations(self, output_dir: str = \"results\"):\n",
    "        print(\"Creating visualizations...\")\n",
    "        Path(output_dir).mkdir(exist_ok=True)\n",
    "\n",
    "        plt.figure(figsize=(15, 10))\n",
    "\n",
    "        k_values = self.results['K'].values\n",
    "        silhouette_scores = self.results['silhouette_score'].values\n",
    "        davies_bouldin_scores = self.results['davies_bouldin_score'].values\n",
    "        calinski_harabasz_scores = self.results['calinski_harabasz_score'].values\n",
    "        inertia_values = self.results['inertia'].values\n",
    "\n",
    "        # 1\n",
    "        plt.subplot(2, 3, 1)\n",
    "        plt.plot(k_values, silhouette_scores, 'o-', label='Silhouette (cosine)', linewidth=2)\n",
    "        plt.plot(k_values, davies_bouldin_scores, 's-', label='Davies-Bouldin', linewidth=2)\n",
    "        plt.xlabel('Number of Clusters (K)')\n",
    "        plt.ylabel('Score')\n",
    "        plt.title('Clustering Quality Metrics')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "\n",
    "        # 2\n",
    "        plt.subplot(2, 3, 2)\n",
    "        plt.plot(k_values, calinski_harabasz_scores, 'o-', linewidth=2)\n",
    "        plt.xlabel('Number of Clusters (K)')\n",
    "        plt.ylabel('Calinski-Harabasz Score')\n",
    "        plt.title('Calinski-Harabasz Index')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "\n",
    "        # 3\n",
    "        plt.subplot(2, 3, 3)\n",
    "        plt.plot(k_values, inertia_values, 'o-', linewidth=2)\n",
    "        plt.xlabel('Number of Clusters (K)')\n",
    "        plt.ylabel('Inertia')\n",
    "        plt.title('KMeans Inertia (Elbow)')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "\n",
    "        # 4 normalized\n",
    "        plt.subplot(2, 3, 4)\n",
    "        sil_norm = (silhouette_scores - silhouette_scores.min()) / max(1e-9, (silhouette_scores.max() - silhouette_scores.min()))\n",
    "        ch_norm = (calinski_harabasz_scores - calinski_harabasz_scores.min()) / max(1e-9, (calinski_harabasz_scores.max() - calinski_harabasz_scores.min()))\n",
    "        plt.plot(k_values, sil_norm, 'o-', label='Silhouette (norm)', linewidth=2)\n",
    "        plt.plot(k_values, ch_norm, 's-', label='CH (norm)', linewidth=2)\n",
    "        plt.xlabel('Number of Clusters (K)')\n",
    "        plt.ylabel('Normalized Score')\n",
    "        plt.title('Normalized Metrics')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "\n",
    "        # 5 label distribution\n",
    "        plt.subplot(2, 3, 5)\n",
    "        self._plot_label_distribution()\n",
    "\n",
    "        # 6 top features\n",
    "        plt.subplot(2, 3, 6)\n",
    "        self._plot_top_features()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{output_dir}/clustering_quality.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"Saved clustering quality plot: {output_dir}/clustering_quality.png\")\n",
    "\n",
    "        self._create_umap_visualization(output_dir)\n",
    "        self._create_label_visualizations(output_dir)\n",
    "\n",
    "    def _plot_label_distribution(self):\n",
    "        all_labels = np.array([lbl for lbl in self.df_processed['binary_labels']], dtype=int)\n",
    "        if all_labels.size == 0:\n",
    "            plt.text(0.5, 0.5, 'No labels', ha='center')\n",
    "            return\n",
    "        label_counts = all_labels.sum(axis=0)\n",
    "        top_k = min(10, self.num_classes)\n",
    "        top_idx = np.argsort(label_counts)[-top_k:][::-1]\n",
    "        top_counts = label_counts[top_idx]\n",
    "        top_labels = [self.class_names[i] for i in top_idx]\n",
    "\n",
    "        plt.bar(range(len(top_labels)), top_counts, alpha=0.7)\n",
    "        plt.xticks(range(len(top_labels)), top_labels, rotation=45, ha='right')\n",
    "        plt.ylabel('Count')\n",
    "        plt.title('Top Label Distribution')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "\n",
    "    def _plot_top_features(self):\n",
    "        feature_names = self.vectorizer.get_feature_names_out()\n",
    "        tfidf_scores = np.asarray(self.X_tfidf.mean(axis=0)).flatten()\n",
    "        top_idx = np.argsort(tfidf_scores)[-10:][::-1]\n",
    "        top_features = [feature_names[i] for i in top_idx]\n",
    "        top_scores = tfidf_scores[top_idx]\n",
    "\n",
    "        plt.barh(range(len(top_features)), top_scores, alpha=0.7)\n",
    "        plt.yticks(range(len(top_features)), top_features)\n",
    "        plt.xlabel('TF-IDF Score')\n",
    "        plt.title('Top TF-IDF Features')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "\n",
    "    # ---------- UMAP with cluster↔label correspondence ----------\n",
    "    def _compute_cluster_major_labels(self, cluster_labels: np.ndarray) -> dict:\n",
    "        \"\"\"\n",
    "        Count most frequent true label (by positive count) in each cluster.\n",
    "        Returns {cluster_id: major_label_index}; -1 if no samples or no positives.\n",
    "        \"\"\"\n",
    "        major_map = {}\n",
    "        unique_cids = np.unique(cluster_labels)\n",
    "        for cid in unique_cids:\n",
    "            mask = (cluster_labels == cid)\n",
    "            sub = self.df_processed[mask]\n",
    "            if len(sub) == 0:\n",
    "                major_map[int(cid)] = -1\n",
    "                continue\n",
    "            cls_arr = np.array([lbl for lbl in sub['binary_labels']], dtype=int)\n",
    "            if cls_arr.size == 0 or cls_arr.sum() == 0:\n",
    "                major_map[int(cid)] = -1\n",
    "                continue\n",
    "            major_map[int(cid)] = int(cls_arr.sum(axis=0).argmax())\n",
    "        return major_map\n",
    "\n",
    "    def _create_umap_visualization(self, output_dir: str):\n",
    "        if not UMAP_AVAILABLE:\n",
    "            print(\"Skipping UMAP visualization (UMAP not available)\")\n",
    "            return\n",
    "        print(\"Creating UMAP visualization (cluster ↔ label correspondence)...\")\n",
    "\n",
    "        best_k_idx = self.results['silhouette_score'].idxmax()\n",
    "        best_k = int(self.results.loc[best_k_idx, 'K'])\n",
    "\n",
    "        kmeans = KMeans(n_clusters=best_k, n_init=10, max_iter=300, random_state=42, algorithm='lloyd')\n",
    "        cluster_labels = kmeans.fit_predict(self.X_tfidf)\n",
    "\n",
    "        svd_dim = min(100, self.X_tfidf.shape[1] - 1) if self.X_tfidf.shape[1] > 1 else 1\n",
    "        svd = TruncatedSVD(n_components=svd_dim, random_state=42)\n",
    "        X_reduced = svd.fit_transform(self.X_tfidf)\n",
    "\n",
    "        reducer = umap.UMAP(n_components=2, random_state=42, n_neighbors=15, min_dist=0.1)\n",
    "        embedding = reducer.fit_transform(X_reduced)\n",
    "\n",
    "        # dominant true label per sample\n",
    "        def _dom(bl):\n",
    "            arr = np.array(bl, dtype=int)\n",
    "            return int(arr.argmax()) if arr.sum() > 0 else -1\n",
    "        dom_idx = [_dom(bl) for bl in self.df_processed['binary_labels']]\n",
    "        dom_name = [self.class_names[i] if (isinstance(i, (int, np.integer)) and 0 <= i < self.num_classes) else 'none'\n",
    "                    for i in dom_idx]\n",
    "\n",
    "        df_umap = pd.DataFrame({\n",
    "            'x': embedding[:, 0],\n",
    "            'y': embedding[:, 1],\n",
    "            'cluster': cluster_labels,\n",
    "            'dominant_label_name': dom_name\n",
    "        })\n",
    "\n",
    "        # major label per cluster\n",
    "        cluster_major = self._compute_cluster_major_labels(cluster_labels)\n",
    "\n",
    "        # ---- View 1: color=cluster ----\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sc = plt.scatter(df_umap['x'], df_umap['y'], c=df_umap['cluster'], cmap='tab10', s=12, alpha=0.75)\n",
    "        plt.colorbar(sc, label='Cluster ID')\n",
    "        plt.title(f'UMAP: Clusters (K={best_k})')\n",
    "        plt.xlabel('UMAP 1'); plt.ylabel('UMAP 2')\n",
    "\n",
    "        for cid in sorted(df_umap['cluster'].unique()):\n",
    "            sub = df_umap[df_umap['cluster'] == cid]\n",
    "            cx, cy = sub['x'].mean(), sub['y'].mean()\n",
    "            plt.text(cx, cy, str(cid), fontsize=10, weight='bold',\n",
    "                     ha='center', va='center',\n",
    "                     bbox=dict(boxstyle='round,pad=0.2', fc='white', alpha=0.65, lw=0.5))\n",
    "            if _HAVE_SCIPY and len(sub) >= 3:\n",
    "                try:\n",
    "                    pts = sub[['x', 'y']].values\n",
    "                    hull = ConvexHull(pts)\n",
    "                    verts = pts[hull.vertices]\n",
    "                    plt.fill(verts[:, 0], verts[:, 1], alpha=0.12)\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{output_dir}/umap_clusters.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"Saved: {output_dir}/umap_clusters.png\")\n",
    "\n",
    "        # ---- View 2: color by dominant true label name ----\n",
    "        label_codes, label_uniques = pd.factorize(df_umap['dominant_label_name'])\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        _ = plt.scatter(df_umap['x'], df_umap['y'], c=label_codes, cmap='tab20', s=12, alpha=0.75)\n",
    "        handles = [Line2D([0], [0], marker='o', linestyle='',\n",
    "                          markersize=6, label=name) for name in label_uniques[:20]]\n",
    "        plt.legend(handles=handles, title=\"Dominant Label (top 20)\", loc='best', fontsize=8)\n",
    "        plt.title('UMAP: Colored by Dominant True Label')\n",
    "        plt.xlabel('UMAP 1'); plt.ylabel('UMAP 2')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{output_dir}/umap_labels.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"Saved: {output_dir}/umap_labels.png\")\n",
    "\n",
    "        # ---- View 3: cluster color; text shows cluster-major label name ----\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        _ = plt.scatter(df_umap['x'], df_umap['y'], c=df_umap['cluster'], cmap='tab10', s=14, alpha=0.75)\n",
    "\n",
    "        for cid in sorted(df_umap['cluster'].unique()):\n",
    "            sub = df_umap[df_umap['cluster'] == cid]\n",
    "            cx, cy = sub['x'].mean(), sub['y'].mean()\n",
    "            maj_idx = cluster_major.get(int(cid), -1)\n",
    "            maj_name = self.class_names[maj_idx] if (isinstance(maj_idx, int) and 0 <= maj_idx < self.num_classes) else 'none'\n",
    "            plt.text(cx, cy, f'{cid}\\n{maj_name}', fontsize=8, weight='bold',\n",
    "                     ha='center', va='center',\n",
    "                     bbox=dict(boxstyle='round,pad=0.25', fc='white', alpha=0.75, lw=0.5))\n",
    "\n",
    "        plt.title('UMAP: Cluster ID with Major True Label Name')\n",
    "        plt.xlabel('UMAP 1'); plt.ylabel('UMAP 2')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{output_dir}/umap_cluster_label_match.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"Saved: {output_dir}/umap_cluster_label_match.png\")\n",
    "\n",
    "    def _create_label_visualizations(self, output_dir: str):\n",
    "        print(\"Creating label visualizations...\")\n",
    "        best_k_idx = self.results['silhouette_score'].idxmax()\n",
    "        best_k = int(self.results.loc[best_k_idx, 'K'])\n",
    "\n",
    "        kmeans = KMeans(n_clusters=best_k, n_init=10, max_iter=300, random_state=42, algorithm='lloyd')\n",
    "        cluster_labels = kmeans.fit_predict(self.X_tfidf)\n",
    "\n",
    "        plt.figure(figsize=(20, 12))\n",
    "\n",
    "        # 1) overall top labels\n",
    "        plt.subplot(2, 3, 1)\n",
    "        all_labels = np.array([lbl for lbl in self.df_processed['binary_labels']], dtype=int)\n",
    "        if all_labels.size > 0:\n",
    "            label_counts = all_labels.sum(axis=0)\n",
    "            top_k = min(15, self.num_classes)\n",
    "            top_indices = np.argsort(label_counts)[-top_k:][::-1]\n",
    "            top_counts = label_counts[top_indices]\n",
    "            top_labels = [self.class_names[i] for i in top_indices]\n",
    "            plt.bar(range(len(top_labels)), top_counts, alpha=0.7)\n",
    "            plt.xticks(range(len(top_labels)), top_labels, rotation=45, ha='right')\n",
    "            plt.ylabel('Count'); plt.title('Top Label Distribution')\n",
    "        else:\n",
    "            plt.text(0.5, 0.5, 'No labels', ha='center')\n",
    "            plt.title('Top Label Distribution')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "\n",
    "        # 2) cluster size\n",
    "        plt.subplot(2, 3, 2)\n",
    "        cluster_sizes = [int(np.sum(cluster_labels == i)) for i in range(best_k)]\n",
    "        plt.bar(range(best_k), cluster_sizes, alpha=0.7)\n",
    "        plt.xlabel('Cluster ID'); plt.ylabel('Num Samples')\n",
    "        plt.title(f'Cluster Size Distribution (K={best_k})')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "\n",
    "        # 3) cluster vs top labels heatmap\n",
    "        plt.subplot(2, 3, 3)\n",
    "        if all_labels.size > 0:\n",
    "            top_k = min(10, self.num_classes)\n",
    "            top_label_indices = np.argsort(all_labels.sum(axis=0))[-top_k:][::-1]\n",
    "            heatmap_data = np.zeros((best_k, len(top_label_indices)), dtype=float)\n",
    "            for cid in range(best_k):\n",
    "                mask = (cluster_labels == cid)\n",
    "                sub = self.df_processed[mask]\n",
    "                cls_arr = np.array([lbl for lbl in sub['binary_labels']], dtype=int)\n",
    "                if cls_arr.size > 0:\n",
    "                    for j, idx in enumerate(top_label_indices):\n",
    "                        heatmap_data[cid, j] = float(cls_arr[:, idx].sum())\n",
    "            im = plt.imshow(heatmap_data, cmap='YlOrRd', aspect='auto')\n",
    "            plt.colorbar(im)\n",
    "            plt.xlabel('Top Labels'); plt.ylabel('Cluster ID')\n",
    "            plt.title('Cluster vs Label Heatmap')\n",
    "            plt.xticks(range(len(top_label_indices)), [self.class_names[i] for i in top_label_indices], rotation=45)\n",
    "            plt.yticks(range(best_k), [f'Cluster {i}' for i in range(best_k)])\n",
    "        else:\n",
    "            plt.text(0.5, 0.5, 'No labels', ha='center')\n",
    "            plt.title('Cluster vs Label Heatmap')\n",
    "\n",
    "        # 4) label co-occurrence (top 5)\n",
    "        plt.subplot(2, 3, 4)\n",
    "        if all_labels.size > 0:\n",
    "            counts = all_labels.sum(axis=0)\n",
    "            top_k = min(5, self.num_classes)\n",
    "            top_5_indices = np.argsort(counts)[-top_k:][::-1]\n",
    "            coocc = np.zeros((top_k, top_k), dtype=float)\n",
    "            for i, a in enumerate(top_5_indices):\n",
    "                for j, b in enumerate(top_5_indices):\n",
    "                    if i == j:\n",
    "                        coocc[i, j] = float(counts[a])\n",
    "                    else:\n",
    "                        coocc[i, j] = float(np.sum((all_labels[:, a] == 1) & (all_labels[:, b] == 1)))\n",
    "            im = plt.imshow(coocc, cmap='Blues', aspect='auto')\n",
    "            plt.colorbar(im)\n",
    "            labs = [self.class_names[i] for i in top_5_indices]\n",
    "            plt.xticks(range(top_k), labs, rotation=45); plt.yticks(range(top_k), labs)\n",
    "            plt.title('Label Co-occurrence')\n",
    "        else:\n",
    "            plt.text(0.5, 0.5, 'No labels', ha='center')\n",
    "            plt.title('Label Co-occurrence')\n",
    "\n",
    "        # 5) cluster purity\n",
    "        plt.subplot(2, 3, 5)\n",
    "        cluster_purities = []\n",
    "        for cid in range(best_k):\n",
    "            mask = (cluster_labels == cid)\n",
    "            sub = self.df_processed[mask]\n",
    "            cls_arr = np.array([lbl for lbl in sub['binary_labels']], dtype=int)\n",
    "            if cls_arr.size > 0:\n",
    "                max_label_count = cls_arr.sum(axis=0).max()\n",
    "                purity = float(max_label_count) / len(cls_arr)\n",
    "            else:\n",
    "                purity = 0.0\n",
    "            cluster_purities.append(purity)\n",
    "        plt.bar(range(best_k), cluster_purities, alpha=0.7)\n",
    "        plt.xlabel('Cluster ID'); plt.ylabel('Purity')\n",
    "        plt.title('Cluster Purity')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "\n",
    "        # 6) label entropy\n",
    "        plt.subplot(2, 3, 6)\n",
    "        label_entropies = []\n",
    "        for cid in range(best_k):\n",
    "            mask = (cluster_labels == cid)\n",
    "            sub = self.df_processed[mask]\n",
    "            cls_arr = np.array([lbl for lbl in sub['binary_labels']], dtype=int)\n",
    "            if cls_arr.size > 0:\n",
    "                total_pos = cls_arr.sum()\n",
    "                if total_pos > 0:\n",
    "                    probs = cls_arr.sum(axis=0) / total_pos\n",
    "                    probs = probs[probs > 0]\n",
    "                    entropy = float(-(probs * np.log2(probs)).sum())\n",
    "                else:\n",
    "                    entropy = 0.0\n",
    "            else:\n",
    "                entropy = 0.0\n",
    "            label_entropies.append(entropy)\n",
    "        plt.bar(range(best_k), label_entropies, alpha=0.7)\n",
    "        plt.xlabel('Cluster ID'); plt.ylabel('Entropy')\n",
    "        plt.title('Cluster Label Entropy')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{output_dir}/label_analysis.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"Saved label analysis: {output_dir}/label_analysis.png\")\n",
    "\n",
    "    # ---------- Report ----------\n",
    "    def generate_report(self, output_dir: str = \"results\"):\n",
    "        print(\"Generating analysis report...\")\n",
    "        lines = []\n",
    "        lines.append(\"MIMIC-CXR Caption TF-IDF Clustering Analysis Report\")\n",
    "        lines.append(\"=\" * 60)\n",
    "        lines.append(\"\")\n",
    "        lines.append(\"Dataset Overview:\")\n",
    "        lines.append(f\"  - Total samples: {len(self.df_processed)}\")\n",
    "        lines.append(f\"  - TF-IDF matrix shape: {self.X_tfidf.shape}\")\n",
    "        lines.append(f\"  - Vocabulary size: {len(self.vectorizer.vocabulary_)}\")\n",
    "        lines.append(f\"  - Num classes: {self.num_classes}\")\n",
    "        show_classes = self.class_names[:20] if self.num_classes <= 20 else self.class_names[:10]\n",
    "        lines.append(f\"  - Class names (partial): {show_classes}\")\n",
    "        lines.append(\"\")\n",
    "        lines.append(\"Clustering Results:\")\n",
    "        lines.append(self.results.to_string(index=False))\n",
    "        lines.append(\"\")\n",
    "        best_idx = self.results['silhouette_score'].idxmax()\n",
    "        best_k = int(self.results.loc[best_idx, 'K'])\n",
    "        best_sil = float(self.results.loc[best_idx, 'silhouette_score'])\n",
    "        lines.append(\"Best K Recommendations:\")\n",
    "        lines.append(f\"  - Best Silhouette Score: K={best_k} (score: {best_sil:.3f})\")\n",
    "        lines.append(\"\")\n",
    "        # Top TF-IDF features\n",
    "        feature_names = self.vectorizer.get_feature_names_out()\n",
    "        tfidf_scores = np.asarray(self.X_tfidf.mean(axis=0)).flatten()\n",
    "        top_idx = np.argsort(tfidf_scores)[-10:][::-1]\n",
    "        lines.append(\"Top 10 TF-IDF Features:\")\n",
    "        for i, j in enumerate(top_idx, start=1):\n",
    "            lines.append(f\"  {i:2d}. {feature_names[j]:<20} (score: {tfidf_scores[j]:.4f})\")\n",
    "\n",
    "        Path(output_dir).mkdir(exist_ok=True)\n",
    "        with open(f'{output_dir}/cluster_analysis_report.txt', 'w', encoding='utf-8') as f:\n",
    "            f.write('\\n'.join(lines))\n",
    "        print(f\"Saved analysis report: {output_dir}/cluster_analysis_report.txt\")\n",
    "\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ANALYSIS SUMMARY\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Dataset: {len(self.df_processed)} captions\")\n",
    "        print(f\"TF-IDF: {self.X_tfidf.shape[0]} docs × {self.X_tfidf.shape[1]} feats\")\n",
    "        print(f\"Best K (Silhouette): {best_k} (score: {best_sil:.3f})\")\n",
    "        print(f\"Results saved to directory: {output_dir}\")\n",
    "\n",
    "    # ---------- Pipeline ----------\n",
    "    def run_full_analysis(self, ks: List[int], min_df: int = 2, max_features: int = 1000,\n",
    "                          output_dir: str = \"results\"):\n",
    "        print(\"Starting MIMIC-CXR Caption TF-IDF Clustering Analysis\")\n",
    "        print(\"=\" * 60)\n",
    "        self.load_data()\n",
    "        self.prepare_labels()\n",
    "        self.vectorize_text(min_df=min_df, max_features=max_features)\n",
    "        results = self.cluster_and_evaluate(ks)\n",
    "        Path(output_dir).mkdir(exist_ok=True)\n",
    "        self.create_visualizations(output_dir)\n",
    "        self.generate_report(output_dir)\n",
    "        results.to_csv(f'{output_dir}/cluster_metrics.csv', index=False)\n",
    "        print(\"\\nAnalysis completed successfully!\")\n",
    "        print(f\"Results saved to directory: {output_dir}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"TF-IDF clustering analysis for MIMIC-CXR captions (named labels)\")\n",
    "    parser.add_argument(\"--csv-path\", type=str, default=\"./data/sampled_1000_data.csv\",\n",
    "                        help=\"Path to original sampled data CSV\")\n",
    "    parser.add_argument(\"--processed-csv-path\", type=str, default=\"./data/processed_text/processed_captions.csv\",\n",
    "                        help=\"Path to processed captions CSV\")\n",
    "    parser.add_argument(\"--ks\", nargs=\"+\", type=int, default=[3, 4, 5, 6, 7, 8],\n",
    "                        help=\"K values for KMeans clustering\")\n",
    "    parser.add_argument(\"--min-df\", type=int, default=2,\n",
    "                        help=\"Minimum document frequency for TF-IDF\")\n",
    "    parser.add_argument(\"--max-features\", type=int, default=1000,\n",
    "                        help=\"Maximum number of TF-IDF features\")\n",
    "    parser.add_argument(\"--output-dir\", type=str, default=\"results\",\n",
    "                        help=\"Output directory for results\")\n",
    "    parser.add_argument(\"--label-scheme\", type=str, choices=[\"auto\", \"radgraph_metric\", \"fg_radgraph\"],\n",
    "                        default=\"auto\", help=\"How to resolve label names\")\n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    if not Path(args.csv_path).exists():\n",
    "        print(f\"CSV file not found: {args.csv_path}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    clusterer = MIMICCaptionClusterer(\n",
    "        args.csv_path,\n",
    "        args.processed_csv_path,\n",
    "        label_scheme=args.label_scheme\n",
    "    )\n",
    "    clusterer.run_full_analysis(\n",
    "        ks=args.ks,\n",
    "        min_df=args.min_df,\n",
    "        max_features=args.max_features,\n",
    "        output_dir=args.output_dir\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82517b37-3f23-4a58-817f-008eb252160a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dl_pytorch)",
   "language": "python",
   "name": "dl_pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
