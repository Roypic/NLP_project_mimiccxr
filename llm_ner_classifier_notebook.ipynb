{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fdea1d9-b1df-470c-ab0e-e33156a16511",
   "metadata": {},
   "source": [
    "# LLM-based NER with Groq "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2603192f-561d-4f10-a6da-2e00b15d44d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[Imports]\n",
    "import os\n",
    "os.environ[\"GROQ_API_KEY\"] = \"YOUR GROQ KEY\"\n",
    "# !pip install groq # As it has free LLMs in comparaison to OPEN AI which needs a paid account\n",
    "# https://console.groq.com/\n",
    "import os, json, time, re, math, random\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb40ee2-d967-430a-ad81-e2e1e89b9923",
   "metadata": {},
   "source": [
    "## Path to Dataset and Groq Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d16f66c9-9743-4215-b6e6-daf97329dea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Has key? True\n",
      "Client is None? False\n",
      "Model: llama-3.1-8b-instant\n"
     ]
    }
   ],
   "source": [
    "# Data files\n",
    "PATH_JSON  = r\"sentence_chunk_Dawood_Annotated.json\"\n",
    "PATH_TEXT  = r\"sentence_chunk_Dawood.txt\"\n",
    "\n",
    "# Project labels\n",
    "LABELS     = [\"OTHER\", \"FINDING\", \"ANATOMY\", \"LOCATION\", \"DESCRIPTION\"]\n",
    "\n",
    "# Choose Groq model \n",
    "GROQ_MODEL = \"llama-3.1-8b-instant\"\n",
    "\n",
    "# Groq client\n",
    "try:\n",
    "    from groq import Groq\n",
    "    groq_client = Groq(api_key = os.getenv(\"GROQ_API_KEY\"))\n",
    "    _           = groq_client  # to avoid lint warnings\n",
    "except Exception as e:\n",
    "    groq_client = None\n",
    "    print(\"Groq client not available. Install with `%pip install groq` and set GROQ_API_KEY.\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"Has key?\", bool(os.getenv(\"GROQ_API_KEY\")))\n",
    "print(\"Client is None?\", groq_client is None)\n",
    "print(\"Model:\", GROQ_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5ac2b5-b86d-4d54-94d6-d1d7c9e935d5",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3d3dd98-4076-4df9-9de8-acee51f085aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 25 sentences with 175 gold entities.\n"
     ]
    }
   ],
   "source": [
    "# In[Load the Dataset]\n",
    "with open(PATH_JSON, \"r\") as f:\n",
    "    annotation = json.load(f)\n",
    "\n",
    "with open(PATH_TEXT, \"r\") as f:\n",
    "    rawText = f.read()\n",
    "\n",
    "n_ents       = 0\n",
    "n_sent       = len(annotation[\"annotations\"])\n",
    "for sent, meta in annotation[\"annotations\"]:\n",
    "    \n",
    "    entities = meta.get(\"entities\", [])\n",
    "    n_ents  += len(entities)\n",
    "\n",
    "print(f\"Loaded {n_sent} sentences with {n_ents} gold entities.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c73d8a-c0dd-4062-8e7a-c965ef42701b",
   "metadata": {},
   "source": [
    "## Prompt, LLM caller, and evaluation utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7082bebb-745c-4b7e-adc9-087ada6ce7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are an information extraction model for radiology sentences.\n",
    "Extract entity spans and classify each span into one of:\n",
    "OTHER, FINDING, ANATOMY, LOCATION, DESCRIPTION.\n",
    "\n",
    "Return ONLY a strict JSON object with this schema:\n",
    "{\n",
    "  \"entities\": [\n",
    "    {\"start\": <int>, \"end\": <int>, \"label\": \"<one of [OTHER,FINDING,ANATOMY,LOCATION,DESCRIPTION]>\"}\n",
    "  ]\n",
    "}\n",
    "Guidelines:\n",
    "- 'start' and 'end' are character offsets over the ORIGINAL input string, using Python slicing semantics; 'end' is exclusive.\n",
    "- The substring must exactly equal the characters between start and end.\n",
    "- If nothing is found, return {\"entities\": []}.\n",
    "- Do NOT include any text outside the JSON.\n",
    "\"\"\"\n",
    "\n",
    "def call_groq_llm(_sentence: str, _mdl: str = \"llama-3.1-70b-versatile\", max_retries: int = 4, min_backoff: float = 0.5) -> Dict[str, Any]:\n",
    "    \n",
    "    \n",
    "    if groq_client is None: # If user forgot to set the API key, then skip LLM and return no entities.\n",
    "        return {\"entities\": []}\n",
    "\n",
    "    _msg = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                {\"role\": \"user\", \"content\": f\"Sentence: {_sentence}\\nReturn JSON only.\"},]\n",
    "    \n",
    "    last_err = None\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            _req     = groq_client.chat.completions.create(model = _mdl, temperature = 0, messages = _msg,)\n",
    "            _content = _req.choices[0].message.content\n",
    "            \n",
    "            # Extract JSON\n",
    "            _srt     = _content.find(\"{\")\n",
    "            _end     = _content.rfind(\"}\")\n",
    "            \n",
    "            if _srt == -1 or _end == -1:\n",
    "                return {\"entities\": []}\n",
    "            _data    = json.loads(_content[_srt:_end + 1]) # Converts .json into a python dict\n",
    "           \n",
    "            if \"entities\" not in _data or not isinstance(_data[\"entities\"], list):\n",
    "                return {\"entities\": []}\n",
    "            \n",
    "                \n",
    "            _cleaned  = []\n",
    "            # But because LLMs sometimes produce invalid positions, the foor loop below verifies the output.\n",
    "\n",
    "            for _ele in _data[\"entities\"]:\n",
    "                try:\n",
    "                    _s, _e = int(_ele[\"start\"]), int(_ele[\"end\"])\n",
    "                    _lab   = str(_ele[\"label\"]).upper()\n",
    "                    \n",
    "                    if 0 <= _s <= _e <= len(_sentence) and _lab in LABELS:\n",
    "                        _cleaned.append({\"start\": _s, \"end\": _e, \"label\": _lab})\n",
    "                except Exception:\n",
    "                    pass\n",
    "            return {\"entities\": _cleaned}\n",
    "        \n",
    "        except Exception as _ele:\n",
    "            last_err = _ele\n",
    "            # basic exponential backoff\n",
    "            time.sleep(min_backoff * (2 ** attempt) + random.random()*0.25) #time delay funtion to avoid overloading the server\n",
    "    # on persistent failure\n",
    "    print(\"LLM call failed after retries:\", last_err)\n",
    "    return {\"entities\": []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99a224d8-4750-41a7-8462-528c22c2b85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Compares the annotated entities against the predicted ones and computes :\n",
    "    - True & False Positive \n",
    "    - False Negatives\n",
    "    - Precision \n",
    "    - Recall and F1-score\n",
    "\"\"\"\n",
    "def evaluatellm(_y : List[Tuple[int,int,str]], _yHat: List[Tuple[int,int,str]]):\n",
    "    _truth, _pred = set(_y), set(_yHat)\n",
    "    _tp = len(_truth & _pred) \n",
    "    _fp = len(_pred - _truth)\n",
    "    _fn = len(_truth - _pred)\n",
    "    _p = _tp/(_tp + _fp) if ( _tp + _fp) else 0.0\n",
    "    _r = _tp/(_tp + _fn) if (_tp + _fn) else 0.0\n",
    "    _f1 = (2 * _p * _r)/(_p + _r) if (_p + _r) else 0.0\n",
    "    return {\"tp\":_tp,\"fp\":_fp,\"fn\":_fn,\"precision\":_p,\"recall\":_r,\"f1\":_f1}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ff42da-f9d5-47ae-9048-c782dca826d0",
   "metadata": {},
   "source": [
    "## Run LLM NER and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0aca6e6a-a3a8-409f-b433-e82792fbb574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro over sentences — Exact span+label: {'precision': 0.024, 'recall': 0.016, 'f1': 0.019}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tp</th>\n",
       "      <th>fp</th>\n",
       "      <th>fn</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DESCRIPTION</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>36</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ANATOMY</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>35</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OTHER</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>49</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FINDING</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>40</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LOCATION</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>13</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         label  tp  fp  fn  precision  recall     f1\n",
       "1  DESCRIPTION   1  10  36      0.091   0.027  0.042\n",
       "3      ANATOMY   1  33  35      0.029   0.028  0.029\n",
       "0        OTHER   0  17  49      0.000   0.000  0.000\n",
       "2      FINDING   0  37  40      0.000   0.000  0.000\n",
       "4     LOCATION   0  26  13      0.000   0.000  0.000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_rows  = []\n",
    "per_label = defaultdict(lambda: {\"tp\" : 0,\"fp\" : 0,\"fn\" : 0})\n",
    "\n",
    "for sent, meta in annotation[\"annotations\"]:\n",
    "    \n",
    "    # gold               = [(int(s), int(e), str(lab)) for (s,e,lab) in meta.get(\"entities\", [])]\n",
    "    gold = []\n",
    "    pred = []\n",
    "\n",
    "    for entity in meta.get(\"entities\", []):\n",
    "        s, e, lab      = entity        # unpack the tuple\n",
    "        s              = int(s)\n",
    "        e              = int(e)\n",
    "        lab            = str(lab)\n",
    "        gold.append((s, e, lab))\n",
    "    \n",
    "    pred_json          = call_groq_llm(sent, _mdl = GROQ_MODEL, max_retries = 2) # Call the LLM to get predictions\n",
    "    \n",
    "    # pred               = [(int(e[\"start\"]), int(e[\"end\"]), str(e[\"label\"])) for e in pred_json.get(\"entities\", [])]\n",
    "    \n",
    "    for ent in pred_json.get(\"entities\", []):\n",
    "        s = int(ent[\"start\"])\n",
    "        e = int(ent[\"end\"])\n",
    "        lab = str(ent[\"label\"])\n",
    "        pred.append((s, e, lab))    \n",
    "    \n",
    "    \n",
    "    res                = evaluatellm(gold, pred)\n",
    "\n",
    "    all_rows.append({\n",
    "        \"sentence\": sent,\n",
    "        \"gold_n\": len(gold),\n",
    "        \"pred_n\": len(pred),\n",
    "        \"p_exact\": round(res[\"precision\"],3),\n",
    "        \"r_exact\": round(res[\"recall\"],3),\n",
    "        \"f1_exact\": round(res[\"f1\"],3),\n",
    "    })\n",
    "\n",
    "    gold_set, pred_set = set(gold), set(pred)\n",
    "    inter              = gold_set & pred_set\n",
    "    labels             = set([g[2] for g in gold] + [m[2] for m in pred])\n",
    "    for lab in labels:\n",
    "        tp = sum(1 for s in inter if s[2]==lab)\n",
    "        fp = sum(1 for s in (pred_set - gold_set) if s[2]==lab)\n",
    "        fn = sum(1 for s in (gold_set - pred_set) if s[2]==lab)\n",
    "        per_label[lab][\"tp\"] += tp\n",
    "        per_label[lab][\"fp\"] += fp\n",
    "        per_label[lab][\"fn\"] += fn\n",
    "\n",
    "df_sent     = pd.DataFrame(all_rows)\n",
    "macro_exact = {\"precision\" : round(df_sent[\"p_exact\"].mean(),3),\n",
    "               \"recall\"    : round(df_sent[\"r_exact\"].mean(),3),\n",
    "               \"f1\"        : round(df_sent[\"f1_exact\"].mean(),3),}\n",
    "\n",
    "print(\"Macro over sentences — Exact span+label:\", macro_exact)\n",
    "\n",
    "# Compute the precision - recall - F1 Score and store them in the form of a df\n",
    "per_label_rows = []\n",
    "for lab, m in per_label.items():\n",
    "    tp, fp, fn = m[\"tp\"], m[\"fp\"], m[\"fn\"]\n",
    "    p          = tp/(tp+fp) if (tp+fp) else 0.0\n",
    "    r          = tp/(tp+fn) if (tp+fn) else 0.0\n",
    "    f          = (2*p*r)/(p+r) if (p+r) else 0.0\n",
    "    per_label_rows.append({ \"label\": lab, \"tp\": tp, \"fp\": fp, \"fn\": fn,\n",
    "                            \"precision\": round(p,3), \"recall\": round(r,3), \"f1\": round(f,3)})\n",
    "    \n",
    "df_labels       = pd.DataFrame(per_label_rows).sort_values(\"f1\", ascending = False)\n",
    "df_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8be57e6-e8fd-4b25-9fb3-4e3b64f0ff3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
